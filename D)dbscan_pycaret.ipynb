{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vvsV193KUN"
      },
      "source": [
        "# DBSCAN Clustering with PyCaret\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates DBSCAN (Density-Based Spatial Clustering of Applications with Noise) using the PyCaret library. DBSCAN is a density-based clustering algorithm that can discover clusters of arbitrary shapes and identify outliers.\n",
        "\n",
        "## Key Concepts\n",
        "- **Epsilon (eps)**: Maximum distance between two points to be considered neighbors\n",
        "- **MinPts (min_samples)**: Minimum number of points to form a dense region (cluster)\n",
        "- **Core Points**: Points with at least MinPts neighbors within eps\n",
        "- **Border Points**: Points within eps of a core point but not core points themselves\n",
        "- **Noise Points**: Points that are neither core nor border points\n",
        "\n",
        "## PyCaret Advantages\n",
        "- Low-code machine learning library\n",
        "- Automated preprocessing\n",
        "- Easy model comparison and evaluation\n",
        "- Built-in visualization functions\n",
        "\n",
        "## Author\n",
        "Created for Machine Learning Course Assignment\n",
        "\n",
        "## References\n",
        "- [PyCaret Clustering](https://pycaret.org/create-model/)\n",
        "- [PyCaret Documentation](http://www.pycaret.org/tutorials/html/CLU101.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsYKjgSA3KUP"
      },
      "source": [
        "## 1. Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH2XNWEH3KUP"
      },
      "outputs": [],
      "source": [
        "# Install PyCaret if not already installed\n",
        "!pip install pycaret[full] --quiet\n",
        "\n",
        "# Additional imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed and plotting style\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Base libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO3k69aI3KUQ"
      },
      "outputs": [],
      "source": [
        "# Import PyCaret\n",
        "from pycaret.clustering import *\n",
        "print(\"PyCaret imported successfully!\")\n",
        "print(f\"PyCaret version: \", end=\"\")\n",
        "import pycaret\n",
        "print(pycaret.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg7WbNMT3KUQ"
      },
      "source": [
        "## 2. Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N3dR4wJ3KUQ"
      },
      "outputs": [],
      "source": [
        "# Generate different types of clusters\n",
        "\n",
        "# 1. Blob clusters\n",
        "X_blobs, y_blobs = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)\n",
        "\n",
        "# 2. Moon-shaped clusters\n",
        "X_moons, y_moons = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "\n",
        "# 3. Concentric circles\n",
        "X_circles, y_circles = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=42)\n",
        "\n",
        "# 4. Clusters with noise\n",
        "X_noise, y_noise = make_blobs(n_samples=250, centers=3, cluster_std=0.4, random_state=42)\n",
        "# Add noise points\n",
        "noise_points = np.random.uniform(low=-10, high=10, size=(50, 2))\n",
        "X_with_noise = np.vstack([X_noise, noise_points])\n",
        "y_with_noise = np.concatenate([y_noise, [-1] * 50])  # -1 for noise\n",
        "\n",
        "# Visualize all datasets\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "datasets = [\n",
        "    (X_blobs, y_blobs, 'Blob Clusters'),\n",
        "    (X_moons, y_moons, 'Moon Clusters'),\n",
        "    (X_circles, y_circles, 'Concentric Circles'),\n",
        "    (X_with_noise, y_with_noise, 'Clusters with Noise')\n",
        "]\n",
        "\n",
        "for ax, (X, y, title) in zip(axes.flatten(), datasets):\n",
        "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7, edgecolors='k', s=50)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.suptitle('Synthetic Datasets for DBSCAN', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AgcnFbl3KUQ"
      },
      "source": [
        "## 3. Understanding DBSCAN Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd5iYyTo3KUR"
      },
      "outputs": [],
      "source": [
        "# Elbow method to find optimal epsilon using k-distance graph\n",
        "def find_optimal_eps(X, min_samples=5):\n",
        "    \"\"\"\n",
        "    Uses the k-distance graph method to find optimal epsilon.\n",
        "    The 'elbow' in the graph suggests a good eps value.\n",
        "    \"\"\"\n",
        "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
        "    neighbors.fit(X)\n",
        "    distances, _ = neighbors.kneighbors(X)\n",
        "    distances = np.sort(distances[:, min_samples-1])\n",
        "    return distances\n",
        "\n",
        "# Plot k-distance graphs for different datasets\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "for ax, (X, y, title) in zip(axes.flatten(), datasets):\n",
        "    distances = find_optimal_eps(X, min_samples=5)\n",
        "    ax.plot(range(len(distances)), distances, 'b-', linewidth=2)\n",
        "    ax.set_xlabel('Points (sorted by distance)')\n",
        "    ax.set_ylabel('5-NN Distance')\n",
        "    ax.set_title(f'K-Distance Graph: {title}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Finding Optimal Epsilon with K-Distance Graph', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThe 'elbow' point in each graph suggests the optimal eps value.\")\n",
        "print(\"Points to the right of the elbow are likely noise.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtOBQz4o3KUR"
      },
      "source": [
        "## 4. DBSCAN with Sklearn (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCzt3Xie3KUR"
      },
      "outputs": [],
      "source": [
        "# Apply DBSCAN with different parameters on moon data\n",
        "eps_values = [0.1, 0.2, 0.3, 0.5]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, eps in enumerate(eps_values):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    labels = dbscan.fit_predict(X_moons)\n",
        "\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise = (labels == -1).sum()\n",
        "\n",
        "    # Plot\n",
        "    unique_labels = set(labels)\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))\n",
        "\n",
        "    for label, color in zip(unique_labels, colors):\n",
        "        if label == -1:\n",
        "            color = 'black'\n",
        "            marker = 'x'\n",
        "        else:\n",
        "            marker = 'o'\n",
        "\n",
        "        mask = labels == label\n",
        "        axes[idx].scatter(X_moons[mask, 0], X_moons[mask, 1], c=[color],\n",
        "                          marker=marker, s=50, alpha=0.7, edgecolors='k' if marker == 'o' else None)\n",
        "\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].set_title(f'eps={eps}\\nClusters: {n_clusters}, Noise: {n_noise}')\n",
        "\n",
        "plt.suptitle('DBSCAN with Different Epsilon Values on Moon Data', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3T9GAg83KUS"
      },
      "source": [
        "## 5. Using PyCaret for Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH-dSQLr3KUS"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame for PyCaret\n",
        "df_moons = pd.DataFrame(X_moons, columns=['feature_1', 'feature_2'])\n",
        "df_moons['true_label'] = y_moons\n",
        "\n",
        "print(\"Moon Dataset:\")\n",
        "print(df_moons.head())\n",
        "print(f\"\\nShape: {df_moons.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcJsNjrX3KUS"
      },
      "outputs": [],
      "source": [
        "# Initialize PyCaret clustering experiment\n",
        "# We ignore 'true_label' since it's for validation only\n",
        "exp = setup(\n",
        "    data=df_moons,\n",
        "    ignore_features=['true_label'],\n",
        "    normalize=True,\n",
        "    session_id=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"PyCaret setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPdmewVz3KUS"
      },
      "outputs": [],
      "source": [
        "# Create DBSCAN model using PyCaret\n",
        "dbscan_model = create_model('dbscan', eps=0.2, min_samples=5)\n",
        "\n",
        "print(\"\\nDBSCAN Model Created!\")\n",
        "print(dbscan_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kqVeW9a3KUS"
      },
      "outputs": [],
      "source": [
        "# Assign cluster labels\n",
        "dbscan_results = assign_model(dbscan_model)\n",
        "\n",
        "print(\"\\nCluster Assignments:\")\n",
        "print(dbscan_results.head(10))\n",
        "\n",
        "# Cluster distribution\n",
        "print(\"\\nCluster Distribution:\")\n",
        "print(dbscan_results['Cluster'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6iWdwz93KUS"
      },
      "outputs": [],
      "source": [
        "# Visualize DBSCAN results using PyCaret\n",
        "try:\n",
        "    plot_model(dbscan_model, plot='cluster')\n",
        "except:\n",
        "    # Fallback manual visualization\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(dbscan_results['feature_1'], dbscan_results['feature_2'],\n",
        "                          c=dbscan_results['Cluster'].astype('category').cat.codes,\n",
        "                          cmap='viridis', alpha=0.7, edgecolors='k', s=50)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('DBSCAN Clustering Results (PyCaret)')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0cBkfr3KUS"
      },
      "source": [
        "## 6. Comparing Multiple Clustering Models with PyCaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72bXvtcP3KUS"
      },
      "outputs": [],
      "source": [
        "# Create a new dataset with blobs for model comparison\n",
        "df_blobs = pd.DataFrame(X_blobs, columns=['feature_1', 'feature_2'])\n",
        "df_blobs['true_label'] = y_blobs\n",
        "\n",
        "# Setup for blobs dataset\n",
        "exp_blobs = setup(\n",
        "    data=df_blobs,\n",
        "    ignore_features=['true_label'],\n",
        "    normalize=True,\n",
        "    session_id=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiWo9J8O3KUS"
      },
      "outputs": [],
      "source": [
        "# Create multiple models\n",
        "models_to_compare = ['kmeans', 'dbscan', 'hclust', 'meanshift']\n",
        "model_results = {}\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    try:\n",
        "        if model_name == 'kmeans':\n",
        "            model = create_model(model_name, num_clusters=3)\n",
        "        elif model_name == 'dbscan':\n",
        "            model = create_model(model_name, eps=0.5, min_samples=5)\n",
        "        elif model_name == 'hclust':\n",
        "            model = create_model(model_name, num_clusters=3)\n",
        "        else:\n",
        "            model = create_model(model_name)\n",
        "\n",
        "        results = assign_model(model)\n",
        "        model_results[model_name] = {\n",
        "            'model': model,\n",
        "            'results': results\n",
        "        }\n",
        "        print(f\"Created {model_name} model successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating {model_name}: {e}\")\n",
        "\n",
        "print(f\"\\nSuccessfully created {len(model_results)} models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCjzV6qk3KUS"
      },
      "outputs": [],
      "source": [
        "# Visualize all models\n",
        "n_models = len(model_results)\n",
        "fig, axes = plt.subplots(1, n_models + 1, figsize=(4 * (n_models + 1), 4))\n",
        "\n",
        "# Ground truth\n",
        "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis',\n",
        "                alpha=0.7, edgecolors='k', s=50)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('Ground Truth')\n",
        "\n",
        "# Each model\n",
        "for idx, (model_name, data) in enumerate(model_results.items(), 1):\n",
        "    results = data['results']\n",
        "    labels = results['Cluster'].astype('category').cat.codes\n",
        "\n",
        "    # Calculate ARI if possible\n",
        "    try:\n",
        "        ari = adjusted_rand_score(y_blobs, labels)\n",
        "        title = f'{model_name.upper()}\\nARI: {ari:.3f}'\n",
        "    except:\n",
        "        title = model_name.upper()\n",
        "\n",
        "    axes[idx].scatter(results['feature_1'], results['feature_2'], c=labels,\n",
        "                      cmap='viridis', alpha=0.7, edgecolors='k', s=50)\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].set_title(title)\n",
        "\n",
        "plt.suptitle('Comparison of Clustering Algorithms (PyCaret)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62-okwN3KUS"
      },
      "source": [
        "## 7. DBSCAN on Real Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owri__1h3KUT"
      },
      "outputs": [],
      "source": [
        "# Load a real dataset - we'll use the Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df_iris['species'] = iris.target\n",
        "\n",
        "print(\"Iris Dataset:\")\n",
        "print(df_iris.head())\n",
        "print(f\"\\nShape: {df_iris.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lpZqsDw3KUT"
      },
      "outputs": [],
      "source": [
        "# Setup PyCaret for Iris\n",
        "exp_iris = setup(\n",
        "    data=df_iris,\n",
        "    ignore_features=['species'],\n",
        "    normalize=True,\n",
        "    session_id=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Find optimal eps for Iris\n",
        "scaler = StandardScaler()\n",
        "X_iris_scaled = scaler.fit_transform(iris.data)\n",
        "\n",
        "distances = find_optimal_eps(X_iris_scaled, min_samples=5)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(distances)), distances, 'b-', linewidth=2)\n",
        "plt.xlabel('Points (sorted by distance)')\n",
        "plt.ylabel('5-NN Distance')\n",
        "plt.title('K-Distance Graph for Iris Dataset')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axhline(y=0.5, color='red', linestyle='--', label='Suggested eps â‰ˆ 0.5')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYYjMaDc3KUT"
      },
      "outputs": [],
      "source": [
        "# Create DBSCAN model for Iris\n",
        "dbscan_iris = create_model('dbscan', eps=0.5, min_samples=5)\n",
        "\n",
        "# Assign labels\n",
        "iris_results = assign_model(dbscan_iris)\n",
        "\n",
        "print(\"\\nIris DBSCAN Cluster Distribution:\")\n",
        "print(iris_results['Cluster'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsb-Q7B3KUT"
      },
      "outputs": [],
      "source": [
        "# Evaluate clustering quality\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Convert cluster labels\n",
        "cluster_labels = iris_results['Cluster'].astype('category').cat.codes\n",
        "valid_mask = cluster_labels >= 0  # Exclude noise for some metrics\n",
        "\n",
        "# Calculate metrics\n",
        "if valid_mask.sum() > 1 and len(set(cluster_labels[valid_mask])) > 1:\n",
        "    sil_score = silhouette_score(X_iris_scaled[valid_mask], cluster_labels[valid_mask])\n",
        "    ari_score = adjusted_rand_score(iris.target, cluster_labels)\n",
        "    print(f\"Silhouette Score (excl. noise): {sil_score:.4f}\")\n",
        "    print(f\"Adjusted Rand Index: {ari_score:.4f}\")\n",
        "else:\n",
        "    print(\"Not enough valid clusters for metrics calculation\")\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# True labels\n",
        "scatter1 = axes[0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=iris.target,\n",
        "                           cmap='viridis', alpha=0.7, edgecolors='k', s=60)\n",
        "axes[0].set_xlabel('First Principal Component')\n",
        "axes[0].set_ylabel('Second Principal Component')\n",
        "axes[0].set_title('Iris - True Species')\n",
        "\n",
        "# DBSCAN labels\n",
        "scatter2 = axes[1].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=cluster_labels,\n",
        "                           cmap='viridis', alpha=0.7, edgecolors='k', s=60)\n",
        "# Mark noise points\n",
        "noise_mask = cluster_labels == -1\n",
        "axes[1].scatter(X_iris_pca[noise_mask, 0], X_iris_pca[noise_mask, 1],\n",
        "                c='red', marker='x', s=100, linewidths=2, label='Noise')\n",
        "axes[1].set_xlabel('First Principal Component')\n",
        "axes[1].set_ylabel('Second Principal Component')\n",
        "axes[1].set_title('Iris - DBSCAN Clustering')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzI0Fj2K3KUT"
      },
      "source": [
        "## 8. Parameter Sensitivity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SrKmjRw3KUT"
      },
      "outputs": [],
      "source": [
        "# Analyze how eps and min_samples affect clustering\n",
        "eps_range = [0.3, 0.5, 0.7, 0.9]\n",
        "min_samples_range = [3, 5, 7, 10]\n",
        "\n",
        "fig, axes = plt.subplots(len(min_samples_range), len(eps_range), figsize=(16, 16))\n",
        "\n",
        "for i, min_samples in enumerate(min_samples_range):\n",
        "    for j, eps in enumerate(eps_range):\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_iris_scaled)\n",
        "\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "        n_noise = (labels == -1).sum()\n",
        "\n",
        "        # Plot\n",
        "        scatter = axes[i, j].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1],\n",
        "                                      c=labels, cmap='viridis', alpha=0.7, s=30)\n",
        "        axes[i, j].set_title(f'eps={eps}, min_samples={min_samples}\\n'\n",
        "                             f'Clusters: {n_clusters}, Noise: {n_noise}', fontsize=9)\n",
        "        axes[i, j].set_xlabel('PC1', fontsize=8)\n",
        "        axes[i, j].set_ylabel('PC2', fontsize=8)\n",
        "\n",
        "plt.suptitle('DBSCAN Parameter Sensitivity on Iris Dataset', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2upelLl3KUT"
      },
      "source": [
        "## 9. DBSCAN for Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3QDvbkd3KUT"
      },
      "outputs": [],
      "source": [
        "# Use DBSCAN for anomaly detection\n",
        "# Points classified as noise (-1) are potential anomalies\n",
        "\n",
        "# Create dataset with anomalies\n",
        "df_anomaly = pd.DataFrame(X_with_noise, columns=['feature_1', 'feature_2'])\n",
        "df_anomaly['true_anomaly'] = (y_with_noise == -1).astype(int)\n",
        "\n",
        "# Setup PyCaret\n",
        "exp_anomaly = setup(\n",
        "    data=df_anomaly,\n",
        "    ignore_features=['true_anomaly'],\n",
        "    normalize=True,\n",
        "    session_id=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Create DBSCAN model\n",
        "dbscan_anomaly = create_model('dbscan', eps=0.5, min_samples=5)\n",
        "anomaly_results = assign_model(dbscan_anomaly)\n",
        "\n",
        "# Identify anomalies (noise points)\n",
        "anomaly_results['predicted_anomaly'] = (anomaly_results['Cluster'] == 'Noise').astype(int)\n",
        "\n",
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "cm = confusion_matrix(df_anomaly['true_anomaly'], anomaly_results['predicted_anomaly'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Visualization\n",
        "normal_mask = anomaly_results['predicted_anomaly'] == 0\n",
        "anomaly_mask = anomaly_results['predicted_anomaly'] == 1\n",
        "\n",
        "axes[0].scatter(anomaly_results.loc[normal_mask, 'feature_1'],\n",
        "                anomaly_results.loc[normal_mask, 'feature_2'],\n",
        "                c='blue', alpha=0.5, s=30, label='Normal')\n",
        "axes[0].scatter(anomaly_results.loc[anomaly_mask, 'feature_1'],\n",
        "                anomaly_results.loc[anomaly_mask, 'feature_2'],\n",
        "                c='red', marker='x', s=100, linewidths=2, label='Anomaly')\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('DBSCAN Anomaly Detection')\n",
        "axes[0].legend()\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "import seaborn as sns\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('True')\n",
        "axes[1].set_title('Confusion Matrix')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(df_anomaly['true_anomaly'], anomaly_results['predicted_anomaly'],\n",
        "                           target_names=['Normal', 'Anomaly']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hba-Efg83KUT"
      },
      "source": [
        "## 10. Clustering Quality Metrics Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilROVQZa3KUT"
      },
      "outputs": [],
      "source": [
        "# Comprehensive metrics for DBSCAN on different datasets\n",
        "def evaluate_dbscan(X, y_true, eps, min_samples, dataset_name):\n",
        "    \"\"\"\n",
        "    Evaluate DBSCAN clustering and return metrics.\n",
        "    \"\"\"\n",
        "    # Scale data\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Apply DBSCAN\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "    n_noise = (labels == -1).sum()\n",
        "\n",
        "    metrics = {\n",
        "        'Dataset': dataset_name,\n",
        "        'eps': eps,\n",
        "        'min_samples': min_samples,\n",
        "        'N Clusters': n_clusters,\n",
        "        'N Noise': n_noise\n",
        "    }\n",
        "\n",
        "    # Silhouette score (excluding noise)\n",
        "    valid_mask = labels >= 0\n",
        "    if valid_mask.sum() > 1 and n_clusters > 1:\n",
        "        metrics['Silhouette'] = silhouette_score(X_scaled[valid_mask], labels[valid_mask])\n",
        "    else:\n",
        "        metrics['Silhouette'] = np.nan\n",
        "\n",
        "    # ARI (if ground truth available)\n",
        "    if y_true is not None:\n",
        "        metrics['ARI'] = adjusted_rand_score(y_true, labels)\n",
        "    else:\n",
        "        metrics['ARI'] = np.nan\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Evaluate on all datasets\n",
        "evaluation_data = [\n",
        "    (X_blobs, y_blobs, 0.5, 5, 'Blobs'),\n",
        "    (X_moons, y_moons, 0.2, 5, 'Moons'),\n",
        "    (X_circles, y_circles, 0.15, 5, 'Circles'),\n",
        "    (iris.data, iris.target, 0.5, 5, 'Iris')\n",
        "]\n",
        "\n",
        "results_list = []\n",
        "for X, y, eps, min_s, name in evaluation_data:\n",
        "    results_list.append(evaluate_dbscan(X, y, eps, min_s, name))\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "\n",
        "print(\"DBSCAN Clustering Quality Metrics:\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1I1T4P3KUT"
      },
      "source": [
        "## 11. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjFg8As43KUT"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DBSCAN CLUSTERING WITH PYCARET - SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. KEY CONCEPTS:\")\n",
        "print(\"   - Density-based clustering algorithm\")\n",
        "print(\"   - Discovers clusters of arbitrary shapes\")\n",
        "print(\"   - Automatically identifies noise/outliers\")\n",
        "print(\"   - Does not require specifying number of clusters\")\n",
        "\n",
        "print(\"\\n2. PARAMETERS:\")\n",
        "print(\"   - eps: Maximum distance between neighbors\")\n",
        "print(\"   - min_samples: Minimum points to form a cluster\")\n",
        "print(\"   - Use k-distance graph to find optimal eps\")\n",
        "\n",
        "print(\"\\n3. PYCARET ADVANTAGES:\")\n",
        "print(\"   - Simple, low-code interface\")\n",
        "print(\"   - Automatic preprocessing and normalization\")\n",
        "print(\"   - Easy model creation and comparison\")\n",
        "print(\"   - Built-in visualization functions\")\n",
        "\n",
        "print(\"\\n4. CLUSTERING QUALITY METRICS USED:\")\n",
        "print(\"   - Silhouette Score (excluding noise points)\")\n",
        "print(\"   - Adjusted Rand Index (with ground truth)\")\n",
        "print(\"   - Number of clusters found\")\n",
        "print(\"   - Number of noise points detected\")\n",
        "\n",
        "print(\"\\n5. BEST USE CASES:\")\n",
        "print(\"   - Non-globular cluster shapes (moons, circles)\")\n",
        "print(\"   - Data with noise/outliers\")\n",
        "print(\"   - When number of clusters is unknown\")\n",
        "print(\"   - Anomaly detection applications\")\n",
        "\n",
        "print(\"\\n6. LIMITATIONS:\")\n",
        "print(\"   - Sensitive to eps parameter\")\n",
        "print(\"   - Struggles with varying density clusters\")\n",
        "print(\"   - Computationally expensive for large datasets\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6aPouu13KUU"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Ester, M., et al. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. KDD.\n",
        "2. PyCaret Documentation: https://pycaret.org/\n",
        "3. Scikit-learn DBSCAN: https://scikit-learn.org/stable/modules/clustering.html#dbscan\n",
        "4. Schubert, E., et al. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM TODS."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}