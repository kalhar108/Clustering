{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdgi8Zfq3Jbr"
      },
      "source": [
        "# Gaussian Mixture Models (GMM) Clustering\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates Gaussian Mixture Models (GMM) for clustering. GMM is a probabilistic model that assumes data is generated from a mixture of several Gaussian distributions with unknown parameters.\n",
        "\n",
        "## Key Concepts\n",
        "- **Soft Clustering**: Unlike K-Means, GMM assigns probabilities to each cluster\n",
        "- **Expectation-Maximization (EM)**: Algorithm used to estimate GMM parameters\n",
        "- **Covariance Types**: Full, tied, diagonal, or spherical covariances\n",
        "- **Model Selection**: Using BIC/AIC to determine optimal number of components\n",
        "\n",
        "## Advantages over K-Means\n",
        "1. Can model elliptical clusters (not just spherical)\n",
        "2. Provides probability estimates for cluster membership\n",
        "3. More flexible covariance structure\n",
        "\n",
        "## Author\n",
        "Created for Machine Learning Course Assignment\n",
        "\n",
        "## References\n",
        "- [Python Data Science Handbook - GMM](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
        "- [Scikit-learn GMM](https://scikit-learn.org/stable/modules/mixture.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hk0MhAc3Jbv"
      },
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cOsddN3Jbv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Ellipse\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_blobs, load_iris, make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed and plotting style\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HduqFsAz3Jbx"
      },
      "source": [
        "## 2. Generate Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWHB49ij3Jbx"
      },
      "outputs": [],
      "source": [
        "# Generate elongated clusters (where GMM excels)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create elongated Gaussian clusters with different covariances\n",
        "n_samples = 400\n",
        "\n",
        "# Cluster 1: Elongated horizontally\n",
        "cov1 = [[3, 0], [0, 0.3]]\n",
        "cluster1 = np.random.multivariate_normal([0, 0], cov1, n_samples // 4)\n",
        "\n",
        "# Cluster 2: Elongated diagonally\n",
        "cov2 = [[2, 1.5], [1.5, 2]]\n",
        "cluster2 = np.random.multivariate_normal([4, 4], cov2, n_samples // 4)\n",
        "\n",
        "# Cluster 3: Nearly spherical\n",
        "cov3 = [[0.5, 0], [0, 0.5]]\n",
        "cluster3 = np.random.multivariate_normal([-2, 4], cov3, n_samples // 4)\n",
        "\n",
        "# Cluster 4: Elongated vertically\n",
        "cov4 = [[0.3, 0], [0, 2]]\n",
        "cluster4 = np.random.multivariate_normal([5, -1], cov4, n_samples // 4)\n",
        "\n",
        "# Combine all clusters\n",
        "X_gmm = np.vstack([cluster1, cluster2, cluster3, cluster4])\n",
        "y_true_gmm = np.array([0] * (n_samples // 4) + [1] * (n_samples // 4) +\n",
        "                       [2] * (n_samples // 4) + [3] * (n_samples // 4))\n",
        "\n",
        "# Also generate standard blob data for comparison\n",
        "X_blobs, y_blobs = make_blobs(n_samples=400, centers=4, cluster_std=0.7, random_state=42)\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_true_gmm, cmap='viridis',\n",
        "                alpha=0.7, edgecolors='k', s=50)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('Elongated Gaussian Clusters\\n(Ideal for GMM)')\n",
        "\n",
        "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_blobs, cmap='viridis',\n",
        "                alpha=0.7, edgecolors='k', s=50)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title('Standard Blob Clusters\\n(Spherical)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Elongated clusters shape: {X_gmm.shape}\")\n",
        "print(f\"Blob clusters shape: {X_blobs.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhFIx_ml3Jby"
      },
      "source": [
        "## 3. Understanding GMM: Covariance Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTXFD1xU3Jby"
      },
      "outputs": [],
      "source": [
        "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
        "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    # Convert covariance to principal axes\n",
        "    if covariance.shape == (2, 2):\n",
        "        U, s, Vt = np.linalg.svd(covariance)\n",
        "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
        "        width, height = 2 * np.sqrt(s)\n",
        "    else:\n",
        "        angle = 0\n",
        "        width, height = 2 * np.sqrt(covariance)\n",
        "\n",
        "    # Draw the ellipse\n",
        "    for nsig in range(1, 4):\n",
        "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle=angle, **kwargs))\n",
        "\n",
        "def plot_gmm(gmm, X, label=True, ax=None):\n",
        "    \"\"\"Plot the GMM cluster assignments and covariance ellipses\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    labels = gmm.fit_predict(X)\n",
        "\n",
        "    if label:\n",
        "        ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', alpha=0.7)\n",
        "    else:\n",
        "        ax.scatter(X[:, 0], X[:, 1], s=40, alpha=0.7)\n",
        "\n",
        "    w_factor = 0.2 / gmm.weights_.max()\n",
        "    for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n",
        "        draw_ellipse(pos, covar, alpha=w * w_factor, ax=ax,\n",
        "                    facecolor='none', edgecolor='red', linewidth=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDqyNNfE3Jb0"
      },
      "outputs": [],
      "source": [
        "# Compare different covariance types\n",
        "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, cov_type in enumerate(covariance_types):\n",
        "    gmm = GaussianMixture(n_components=4, covariance_type=cov_type, random_state=42)\n",
        "    labels = gmm.fit_predict(X_gmm)\n",
        "\n",
        "    # Calculate ARI\n",
        "    ari = adjusted_rand_score(y_true_gmm, labels)\n",
        "\n",
        "    # Plot\n",
        "    plot_gmm(gmm, X_gmm, ax=axes[idx])\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].set_title(f'Covariance Type: {cov_type.capitalize()}\\nARI: {ari:.3f}')\n",
        "\n",
        "plt.suptitle('GMM with Different Covariance Types on Elongated Clusters',\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCovariance Type Descriptions:\")\n",
        "print(\"• Full: Each component has its own general covariance matrix\")\n",
        "print(\"• Tied: All components share the same general covariance matrix\")\n",
        "print(\"• Diag: Each component has its own diagonal covariance matrix\")\n",
        "print(\"• Spherical: Each component has its own single variance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0rfEqCW3Jb0"
      },
      "source": [
        "## 4. Soft Clustering: Probability Assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDWeWFIC3Jb1"
      },
      "outputs": [],
      "source": [
        "# Demonstrate soft clustering\n",
        "gmm_full = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
        "gmm_full.fit(X_gmm)\n",
        "\n",
        "# Get probabilities for each cluster\n",
        "probs = gmm_full.predict_proba(X_gmm)\n",
        "labels = gmm_full.predict(X_gmm)\n",
        "\n",
        "# Display probability distribution for some samples\n",
        "print(\"Soft Clustering - Probability Assignments:\")\n",
        "print(\"=\"*60)\n",
        "prob_df = pd.DataFrame(probs, columns=[f'Cluster {i}' for i in range(4)])\n",
        "prob_df['Assigned Cluster'] = labels\n",
        "prob_df['Max Probability'] = probs.max(axis=1)\n",
        "\n",
        "print(\"\\nSample of probability assignments (first 10 points):\")\n",
        "print(prob_df.head(10).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsZqxcsc3Jb1"
      },
      "outputs": [],
      "source": [
        "# Visualize uncertainty in cluster assignments\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Points colored by cluster\n",
        "scatter1 = axes[0].scatter(X_gmm[:, 0], X_gmm[:, 1], c=labels, cmap='viridis',\n",
        "                           alpha=0.7, edgecolors='k', s=50)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('Hard Cluster Assignments')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
        "\n",
        "# Plot 2: Points sized/colored by uncertainty\n",
        "uncertainty = 1 - probs.max(axis=1)  # Higher uncertainty = lower max probability\n",
        "scatter2 = axes[1].scatter(X_gmm[:, 0], X_gmm[:, 1], c=uncertainty, cmap='Reds',\n",
        "                           alpha=0.7, edgecolors='k', s=50 + 200 * uncertainty)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title('Cluster Assignment Uncertainty\\n(Larger/Darker = More Uncertain)')\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Uncertainty')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistics on uncertainty\n",
        "print(f\"\\nUncertainty Statistics:\")\n",
        "print(f\"  Mean uncertainty: {uncertainty.mean():.4f}\")\n",
        "print(f\"  Max uncertainty: {uncertainty.max():.4f}\")\n",
        "print(f\"  Points with >10% uncertainty: {(uncertainty > 0.1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfsVbhwo3Jb2"
      },
      "source": [
        "## 5. Model Selection: BIC and AIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYEV-EgE3Jb2"
      },
      "outputs": [],
      "source": [
        "# Use BIC and AIC to determine optimal number of components\n",
        "n_components_range = range(1, 10)\n",
        "\n",
        "bic_scores = []\n",
        "aic_scores = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for n in n_components_range:\n",
        "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)\n",
        "    gmm.fit(X_gmm)\n",
        "    bic_scores.append(gmm.bic(X_gmm))\n",
        "    aic_scores.append(gmm.aic(X_gmm))\n",
        "\n",
        "    if n > 1:\n",
        "        labels = gmm.predict(X_gmm)\n",
        "        silhouette_scores.append(silhouette_score(X_gmm, labels))\n",
        "    else:\n",
        "        silhouette_scores.append(0)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "axes[0].plot(n_components_range, bic_scores, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Components')\n",
        "axes[0].set_ylabel('BIC Score')\n",
        "axes[0].set_title('BIC Score vs Number of Components\\n(Lower is Better)')\n",
        "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(n_components_range, aic_scores, 'go-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Components')\n",
        "axes[1].set_ylabel('AIC Score')\n",
        "axes[1].set_title('AIC Score vs Number of Components\\n(Lower is Better)')\n",
        "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Optimal')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(n_components_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "axes[2].set_xlabel('Number of Components')\n",
        "axes[2].set_ylabel('Silhouette Score')\n",
        "axes[2].set_title('Silhouette Score vs Number of Components\\n(Higher is Better)')\n",
        "axes[2].axvline(x=4, color='blue', linestyle='--', alpha=0.7, label='True K=4')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal\n",
        "optimal_bic = list(n_components_range)[np.argmin(bic_scores)]\n",
        "optimal_aic = list(n_components_range)[np.argmin(aic_scores)]\n",
        "optimal_sil = list(n_components_range)[np.argmax(silhouette_scores)]\n",
        "\n",
        "print(f\"\\nOptimal number of components:\")\n",
        "print(f\"  By BIC: {optimal_bic}\")\n",
        "print(f\"  By AIC: {optimal_aic}\")\n",
        "print(f\"  By Silhouette: {optimal_sil}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R6-sMcG3Jb2"
      },
      "source": [
        "## 6. GMM vs K-Means Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYuDnU-03Jb2"
      },
      "outputs": [],
      "source": [
        "# Compare GMM and K-Means on elongated clusters\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Ground truth\n",
        "axes[0].scatter(X_gmm[:, 0], X_gmm[:, 1], c=y_true_gmm, cmap='viridis',\n",
        "                alpha=0.7, edgecolors='k', s=50)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('Ground Truth')\n",
        "\n",
        "# K-Means\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels_km = kmeans.fit_predict(X_gmm)\n",
        "ari_km = adjusted_rand_score(y_true_gmm, labels_km)\n",
        "sil_km = silhouette_score(X_gmm, labels_km)\n",
        "\n",
        "axes[1].scatter(X_gmm[:, 0], X_gmm[:, 1], c=labels_km, cmap='viridis',\n",
        "                alpha=0.7, edgecolors='k', s=50)\n",
        "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "                c='red', marker='X', s=200, edgecolors='black', linewidths=2)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title(f'K-Means\\nARI: {ari_km:.3f}, Silhouette: {sil_km:.3f}')\n",
        "\n",
        "# GMM\n",
        "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
        "labels_gmm = gmm.fit_predict(X_gmm)\n",
        "ari_gmm = adjusted_rand_score(y_true_gmm, labels_gmm)\n",
        "sil_gmm = silhouette_score(X_gmm, labels_gmm)\n",
        "\n",
        "plot_gmm(gmm, X_gmm, ax=axes[2])\n",
        "axes[2].set_xlabel('Feature 1')\n",
        "axes[2].set_ylabel('Feature 2')\n",
        "axes[2].set_title(f'GMM (Full Covariance)\\nARI: {ari_gmm:.3f}, Silhouette: {sil_gmm:.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nComparison Summary:\")\n",
        "print(f\"  K-Means - ARI: {ari_km:.4f}, Silhouette: {sil_km:.4f}\")\n",
        "print(f\"  GMM     - ARI: {ari_gmm:.4f}, Silhouette: {sil_gmm:.4f}\")\n",
        "print(\"\\nGMM with full covariance handles elongated clusters better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaAl5Z6n3Jb3"
      },
      "source": [
        "## 7. Density Estimation with GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpNdqZyp3Jb3"
      },
      "outputs": [],
      "source": [
        "# GMM can also be used for density estimation\n",
        "gmm_density = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
        "gmm_density.fit(X_gmm)\n",
        "\n",
        "# Create a grid for density visualization\n",
        "x_min, x_max = X_gmm[:, 0].min() - 1, X_gmm[:, 0].max() + 1\n",
        "y_min, y_max = X_gmm[:, 1].min() - 1, X_gmm[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                     np.linspace(y_min, y_max, 100))\n",
        "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# Calculate log-likelihood (density) on the grid\n",
        "log_density = gmm_density.score_samples(grid_points)\n",
        "density = np.exp(log_density).reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Contour plot\n",
        "cs = axes[0].contourf(xx, yy, density, levels=20, cmap='Blues', alpha=0.8)\n",
        "axes[0].scatter(X_gmm[:, 0], X_gmm[:, 1], c='red', alpha=0.3, s=20)\n",
        "axes[0].set_xlabel('Feature 1')\n",
        "axes[0].set_ylabel('Feature 2')\n",
        "axes[0].set_title('GMM Density Estimation (Contour)')\n",
        "plt.colorbar(cs, ax=axes[0], label='Density')\n",
        "\n",
        "# 3D surface plot (using imshow for simplicity)\n",
        "im = axes[1].imshow(density, extent=[x_min, x_max, y_min, y_max],\n",
        "                    origin='lower', cmap='viridis', aspect='auto')\n",
        "axes[1].scatter(X_gmm[:, 0], X_gmm[:, 1], c='white', alpha=0.3, s=10)\n",
        "axes[1].set_xlabel('Feature 1')\n",
        "axes[1].set_ylabel('Feature 2')\n",
        "axes[1].set_title('GMM Density Estimation (Heatmap)')\n",
        "plt.colorbar(im, ax=axes[1], label='Density')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIVnLYID3Jb3"
      },
      "source": [
        "## 8. Anomaly Detection with GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6-G7W9A3Jb3"
      },
      "outputs": [],
      "source": [
        "# Use GMM for anomaly detection based on log-likelihood\n",
        "log_likelihood = gmm_density.score_samples(X_gmm)\n",
        "\n",
        "# Define anomalies as points with low likelihood (bottom 5%)\n",
        "threshold = np.percentile(log_likelihood, 5)\n",
        "anomalies = log_likelihood < threshold\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(log_likelihood, bins=50, edgecolor='black', alpha=0.7)\n",
        "plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Threshold (5th percentile): {threshold:.2f}')\n",
        "plt.xlabel('Log-Likelihood')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Log-Likelihood Scores')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_gmm[~anomalies, 0], X_gmm[~anomalies, 1], c='blue',\n",
        "            alpha=0.5, s=30, label='Normal')\n",
        "plt.scatter(X_gmm[anomalies, 0], X_gmm[anomalies, 1], c='red',\n",
        "            alpha=0.8, s=100, marker='x', linewidths=2, label='Anomaly')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Anomaly Detection with GMM')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total samples: {len(X_gmm)}\")\n",
        "print(f\"Anomalies detected: {anomalies.sum()}\")\n",
        "print(f\"Anomaly percentage: {100 * anomalies.sum() / len(X_gmm):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGbyqn5M3Jb3"
      },
      "source": [
        "## 9. Real Dataset: Iris with GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdOsoPdD3Jb4"
      },
      "outputs": [],
      "source": [
        "# Load and prepare Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_iris_scaled = scaler.fit_transform(X_iris)\n",
        "\n",
        "# Apply PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_iris_pca = pca.fit_transform(X_iris_scaled)\n",
        "\n",
        "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuZFMo303Jb4"
      },
      "outputs": [],
      "source": [
        "# Find optimal number of components using BIC\n",
        "n_range = range(1, 10)\n",
        "bic_iris = []\n",
        "aic_iris = []\n",
        "\n",
        "for n in n_range:\n",
        "    gmm = GaussianMixture(n_components=n, covariance_type='full', random_state=42)\n",
        "    gmm.fit(X_iris_scaled)\n",
        "    bic_iris.append(gmm.bic(X_iris_scaled))\n",
        "    aic_iris.append(gmm.aic(X_iris_scaled))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(n_range, bic_iris, 'bo-', label='BIC', linewidth=2, markersize=8)\n",
        "plt.plot(n_range, aic_iris, 'go-', label='AIC', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Selection for Iris Dataset')\n",
        "plt.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='True K=3')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "optimal_bic_iris = list(n_range)[np.argmin(bic_iris)]\n",
        "optimal_aic_iris = list(n_range)[np.argmin(aic_iris)]\n",
        "print(f\"Optimal K by BIC: {optimal_bic_iris}\")\n",
        "print(f\"Optimal K by AIC: {optimal_aic_iris}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oceL2v053Jb4"
      },
      "outputs": [],
      "source": [
        "# Apply GMM with 3 components to Iris\n",
        "gmm_iris = GaussianMixture(n_components=3, covariance_type='full', random_state=42)\n",
        "labels_iris = gmm_iris.fit_predict(X_iris_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "ari_iris = adjusted_rand_score(y_iris, labels_iris)\n",
        "nmi_iris = normalized_mutual_info_score(y_iris, labels_iris)\n",
        "sil_iris = silhouette_score(X_iris_scaled, labels_iris)\n",
        "\n",
        "# Visualize in PCA space\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# True labels\n",
        "scatter1 = axes[0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=y_iris,\n",
        "                           cmap='viridis', alpha=0.7, edgecolors='k', s=60)\n",
        "axes[0].set_xlabel('First Principal Component')\n",
        "axes[0].set_ylabel('Second Principal Component')\n",
        "axes[0].set_title('Iris - True Species Labels')\n",
        "\n",
        "# GMM labels\n",
        "scatter2 = axes[1].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], c=labels_iris,\n",
        "                           cmap='viridis', alpha=0.7, edgecolors='k', s=60)\n",
        "axes[1].set_xlabel('First Principal Component')\n",
        "axes[1].set_ylabel('Second Principal Component')\n",
        "axes[1].set_title(f'Iris - GMM Clustering\\nARI: {ari_iris:.3f}, NMI: {nmi_iris:.3f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nIris GMM Clustering Metrics:\")\n",
        "print(f\"  Adjusted Rand Index: {ari_iris:.4f}\")\n",
        "print(f\"  Normalized Mutual Info: {nmi_iris:.4f}\")\n",
        "print(f\"  Silhouette Score: {sil_iris:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hGPfrhI3Jb4"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for GMM predictions\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_iris, labels_iris)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(3), yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted Cluster')\n",
        "plt.ylabel('True Species')\n",
        "plt.title('Confusion Matrix: GMM Clustering on Iris')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNote: Cluster labels may not align with species labels,\")\n",
        "print(\"but the clustering structure should match.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmd5BLCq3Jb4"
      },
      "source": [
        "## 10. Summary and Conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EnAV3ju3Jb4"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"GAUSSIAN MIXTURE MODELS CLUSTERING - SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. KEY CONCEPTS:\")\n",
        "print(\"   - GMM models data as mixture of Gaussian distributions\")\n",
        "print(\"   - Uses Expectation-Maximization (EM) algorithm\")\n",
        "print(\"   - Provides soft clustering (probability assignments)\")\n",
        "\n",
        "print(\"\\n2. COVARIANCE TYPES:\")\n",
        "print(\"   - Full: Most flexible, each cluster has own covariance\")\n",
        "print(\"   - Tied: All clusters share same covariance\")\n",
        "print(\"   - Diag: Axis-aligned ellipses\")\n",
        "print(\"   - Spherical: Similar to K-Means\")\n",
        "\n",
        "print(\"\\n3. MODEL SELECTION:\")\n",
        "print(\"   - BIC (Bayesian Information Criterion): Penalizes complexity more\")\n",
        "print(\"   - AIC (Akaike Information Criterion): Less penalty\")\n",
        "print(\"   - Lower values indicate better models\")\n",
        "\n",
        "print(\"\\n4. ADVANTAGES OVER K-MEANS:\")\n",
        "print(\"   - Handles elliptical clusters (not just spherical)\")\n",
        "print(\"   - Provides probability estimates\")\n",
        "print(\"   - Can detect overlapping clusters\")\n",
        "print(\"   - Useful for density estimation and anomaly detection\")\n",
        "\n",
        "print(\"\\n5. CLUSTERING QUALITY METRICS USED:\")\n",
        "print(\"   - Silhouette Score\")\n",
        "print(\"   - Adjusted Rand Index\")\n",
        "print(\"   - Normalized Mutual Information\")\n",
        "print(\"   - BIC/AIC for model selection\")\n",
        "\n",
        "print(\"\\n6. APPLICATIONS DEMONSTRATED:\")\n",
        "print(\"   - Clustering with different covariance structures\")\n",
        "print(\"   - Soft clustering with uncertainty estimation\")\n",
        "print(\"   - Density estimation\")\n",
        "print(\"   - Anomaly detection\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADfINwX13Jb5"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society.\n",
        "2. VanderPlas, J. (2016). Python Data Science Handbook - Gaussian Mixture Models.\n",
        "3. Scikit-learn documentation: https://scikit-learn.org/stable/modules/mixture.html\n",
        "4. Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Chapter 9."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}