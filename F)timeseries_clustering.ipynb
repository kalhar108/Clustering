{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W8dDMEc3LnM"
      },
      "source": [
        "# Time Series Clustering with Pretrained Models\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates time series clustering using various approaches including pretrained models and specialized distance metrics. We'll explore different methods to cluster time series data effectively.\n",
        "\n",
        "## Approaches Covered\n",
        "1. **DTW (Dynamic Time Warping)**: Elastic distance measure for time series\n",
        "2. **Feature-Based Clustering**: Extract features from time series, then cluster\n",
        "3. **Shape-Based Clustering**: Using time series shapelets\n",
        "4. **Deep Learning Embeddings**: Using pretrained models for time series representation\n",
        "\n",
        "## Libraries Used\n",
        "- **tslearn**: Time series machine learning library\n",
        "- **sktime**: Unified interface for time series ML\n",
        "- **tsfresh**: Automatic feature extraction\n",
        "\n",
        "## Author\n",
        "Created for Machine Learning Course Assignment\n",
        "\n",
        "## References\n",
        "- [V-MalM Stock Clustering](https://github.com/V-MalM/Stock-Clustering-and-Prediction)\n",
        "- [Time Series Clustering Resources](https://github.com/cure-lab/Awesome-time-series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIqZRw8P3LnP"
      },
      "source": [
        "## 1. Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxODeQu23LnP"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install tslearn --quiet\n",
        "!pip install tsfresh --quiet\n",
        "!pip install yfinance --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed and plotting style\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Base libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aqZXsv3LnQ"
      },
      "outputs": [],
      "source": [
        "# Import time series specific libraries\n",
        "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "from tslearn.metrics import cdist_dtw, dtw\n",
        "from tslearn.barycenters import dtw_barycenter_averaging\n",
        "from tslearn.datasets import CachedDatasets\n",
        "\n",
        "print(\"tslearn imported successfully!\")\n",
        "\n",
        "# Import tsfresh for feature extraction\n",
        "from tsfresh import extract_features\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters, MinimalFCParameters\n",
        "from tsfresh.utilities.dataframe_functions import impute\n",
        "\n",
        "print(\"tsfresh imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtUD7sZu3LnR"
      },
      "source": [
        "## 2. Generate Synthetic Time Series Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBtSfrzo3LnR"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_timeseries(n_samples_per_class=30, length=100):\n",
        "    \"\"\"\n",
        "    Generate synthetic time series data with different patterns.\n",
        "\n",
        "    Patterns:\n",
        "    - Class 0: Sinusoidal pattern\n",
        "    - Class 1: Linear trend with noise\n",
        "    - Class 2: Step pattern\n",
        "    - Class 3: Sawtooth pattern\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    t = np.linspace(0, 4*np.pi, length)\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # Class 0: Sinusoidal\n",
        "    for i in range(n_samples_per_class):\n",
        "        freq = np.random.uniform(0.8, 1.2)\n",
        "        phase = np.random.uniform(0, np.pi)\n",
        "        amplitude = np.random.uniform(0.8, 1.2)\n",
        "        noise = np.random.normal(0, 0.1, length)\n",
        "        signal = amplitude * np.sin(freq * t + phase) + noise\n",
        "        data.append(signal)\n",
        "        labels.append(0)\n",
        "\n",
        "    # Class 1: Linear trend\n",
        "    for i in range(n_samples_per_class):\n",
        "        slope = np.random.uniform(0.3, 0.7)\n",
        "        intercept = np.random.uniform(-0.5, 0.5)\n",
        "        noise = np.random.normal(0, 0.2, length)\n",
        "        signal = slope * np.linspace(0, 2, length) + intercept + noise\n",
        "        data.append(signal)\n",
        "        labels.append(1)\n",
        "\n",
        "    # Class 2: Step pattern\n",
        "    for i in range(n_samples_per_class):\n",
        "        step_point = np.random.randint(30, 70)\n",
        "        step_height = np.random.uniform(0.8, 1.5)\n",
        "        noise = np.random.normal(0, 0.15, length)\n",
        "        signal = np.concatenate([np.zeros(step_point), np.ones(length-step_point) * step_height]) + noise\n",
        "        data.append(signal)\n",
        "        labels.append(2)\n",
        "\n",
        "    # Class 3: Sawtooth pattern\n",
        "    for i in range(n_samples_per_class):\n",
        "        period = np.random.randint(20, 35)\n",
        "        noise = np.random.normal(0, 0.1, length)\n",
        "        signal = (np.arange(length) % period) / period + noise\n",
        "        data.append(signal)\n",
        "        labels.append(3)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Generate data\n",
        "X_ts, y_ts = generate_synthetic_timeseries(n_samples_per_class=30, length=100)\n",
        "print(f\"Time series data shape: {X_ts.shape}\")\n",
        "print(f\"Number of samples per class: 30\")\n",
        "print(f\"Time series length: 100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ehbTR6A3LnS"
      },
      "outputs": [],
      "source": [
        "# Visualize sample time series from each class\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
        "class_names = ['Sinusoidal', 'Linear Trend', 'Step Pattern', 'Sawtooth']\n",
        "colors = ['blue', 'green', 'red', 'orange']\n",
        "\n",
        "for class_idx in range(4):\n",
        "    ax = axes[class_idx // 2, class_idx % 2]\n",
        "    class_mask = y_ts == class_idx\n",
        "\n",
        "    # Plot several samples from this class\n",
        "    for i, ts in enumerate(X_ts[class_mask][:5]):\n",
        "        ax.plot(ts, alpha=0.5, color=colors[class_idx])\n",
        "\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Value')\n",
        "    ax.set_title(f'Class {class_idx}: {class_names[class_idx]}')\n",
        "\n",
        "plt.suptitle('Sample Time Series from Each Class', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Y7gpue3LnT"
      },
      "source": [
        "## 3. Time Series Clustering with DTW (Dynamic Time Warping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLk3qiLe3LnT"
      },
      "outputs": [],
      "source": [
        "# Reshape data for tslearn (n_samples, n_timestamps, n_features)\n",
        "X_ts_reshaped = X_ts.reshape(X_ts.shape[0], X_ts.shape[1], 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = TimeSeriesScalerMeanVariance()\n",
        "X_ts_scaled = scaler.fit_transform(X_ts_reshaped)\n",
        "\n",
        "print(f\"Scaled data shape: {X_ts_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXVDwvfm3LnU"
      },
      "outputs": [],
      "source": [
        "# Apply TimeSeriesKMeans with DTW distance\n",
        "n_clusters = 4\n",
        "\n",
        "# DTW-based K-Means\n",
        "print(\"Training DTW K-Means (this may take a moment)...\")\n",
        "dtw_kmeans = TimeSeriesKMeans(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='dtw',\n",
        "    max_iter=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "labels_dtw = dtw_kmeans.fit_predict(X_ts_scaled)\n",
        "\n",
        "# Euclidean-based K-Means for comparison\n",
        "print(\"Training Euclidean K-Means...\")\n",
        "euclidean_kmeans = TimeSeriesKMeans(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='euclidean',\n",
        "    max_iter=10,\n",
        "    random_state=42\n",
        ")\n",
        "labels_euclidean = euclidean_kmeans.fit_predict(X_ts_scaled)\n",
        "\n",
        "# Soft-DTW K-Means\n",
        "print(\"Training Soft-DTW K-Means...\")\n",
        "soft_dtw_kmeans = TimeSeriesKMeans(\n",
        "    n_clusters=n_clusters,\n",
        "    metric='softdtw',\n",
        "    max_iter=10,\n",
        "    random_state=42\n",
        ")\n",
        "labels_soft_dtw = soft_dtw_kmeans.fit_predict(X_ts_scaled)\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RndTDQio3LnU"
      },
      "outputs": [],
      "source": [
        "# Calculate clustering quality metrics\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "# Flatten for sklearn metrics\n",
        "X_flat = X_ts_scaled.reshape(X_ts_scaled.shape[0], -1)\n",
        "\n",
        "results = {\n",
        "    'DTW K-Means': labels_dtw,\n",
        "    'Euclidean K-Means': labels_euclidean,\n",
        "    'Soft-DTW K-Means': labels_soft_dtw\n",
        "}\n",
        "\n",
        "print(\"Clustering Quality Metrics:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "metrics_data = []\n",
        "for name, labels in results.items():\n",
        "    ari = adjusted_rand_score(y_ts, labels)\n",
        "    nmi = normalized_mutual_info_score(y_ts, labels)\n",
        "    sil = silhouette_score(X_flat, labels)\n",
        "\n",
        "    metrics_data.append({\n",
        "        'Method': name,\n",
        "        'ARI': ari,\n",
        "        'NMI': nmi,\n",
        "        'Silhouette': sil\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE7QczAx3LnU"
      },
      "outputs": [],
      "source": [
        "# Visualize cluster centers\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "cluster_models = [\n",
        "    (dtw_kmeans, 'DTW K-Means'),\n",
        "    (euclidean_kmeans, 'Euclidean K-Means'),\n",
        "    (soft_dtw_kmeans, 'Soft-DTW K-Means')\n",
        "]\n",
        "\n",
        "for ax, (model, title) in zip(axes, cluster_models):\n",
        "    for i, center in enumerate(model.cluster_centers_):\n",
        "        ax.plot(center.ravel(), label=f'Cluster {i}')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Value')\n",
        "    ax.set_title(title)\n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.suptitle('Cluster Centers', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fJw9G0x3LnU"
      },
      "source": [
        "## 4. Feature-Based Time Series Clustering with tsfresh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs8TVq8W3LnU"
      },
      "outputs": [],
      "source": [
        "# Prepare data for tsfresh\n",
        "# tsfresh expects a DataFrame with columns: id, time, value\n",
        "\n",
        "tsfresh_data = []\n",
        "for idx, ts in enumerate(X_ts):\n",
        "    for t, val in enumerate(ts):\n",
        "        tsfresh_data.append({'id': idx, 'time': t, 'value': val})\n",
        "\n",
        "df_tsfresh = pd.DataFrame(tsfresh_data)\n",
        "print(f\"tsfresh DataFrame shape: {df_tsfresh.shape}\")\n",
        "print(df_tsfresh.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtdqj15u3LnV"
      },
      "outputs": [],
      "source": [
        "# Extract features using tsfresh (using MinimalFCParameters for speed)\n",
        "print(\"Extracting features with tsfresh (this may take a moment)...\")\n",
        "\n",
        "extracted_features = extract_features(\n",
        "    df_tsfresh,\n",
        "    column_id='id',\n",
        "    column_sort='time',\n",
        "    default_fc_parameters=MinimalFCParameters(),\n",
        "    disable_progressbar=True\n",
        ")\n",
        "\n",
        "# Impute missing values\n",
        "extracted_features = impute(extracted_features)\n",
        "\n",
        "print(f\"Extracted features shape: {extracted_features.shape}\")\n",
        "print(f\"\\nFeature names (first 10): {list(extracted_features.columns[:10])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Dyegp43LnV"
      },
      "outputs": [],
      "source": [
        "# Apply clustering on extracted features\n",
        "scaler = StandardScaler()\n",
        "X_features_scaled = scaler.fit_transform(extracted_features)\n",
        "\n",
        "# K-Means on features\n",
        "kmeans_features = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "labels_features = kmeans_features.fit_predict(X_features_scaled)\n",
        "\n",
        "# Hierarchical clustering\n",
        "hclust_features = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
        "labels_hclust = hclust_features.fit_predict(X_features_scaled)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Feature-Based Clustering Results:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, labels in [('K-Means (Features)', labels_features),\n",
        "                     ('Hierarchical (Features)', labels_hclust)]:\n",
        "    ari = adjusted_rand_score(y_ts, labels)\n",
        "    nmi = normalized_mutual_info_score(y_ts, labels)\n",
        "    sil = silhouette_score(X_features_scaled, labels)\n",
        "    print(f\"{name:25s}: ARI={ari:.4f}, NMI={nmi:.4f}, Silhouette={sil:.4f}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJRCZYb63LnV"
      },
      "outputs": [],
      "source": [
        "# PCA visualization of feature space\n",
        "pca = PCA(n_components=2)\n",
        "X_features_pca = pca.fit_transform(X_features_scaled)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# True labels\n",
        "scatter1 = axes[0].scatter(X_features_pca[:, 0], X_features_pca[:, 1],\n",
        "                           c=y_ts, cmap='viridis', alpha=0.7, s=50, edgecolors='k')\n",
        "axes[0].set_xlabel('First Principal Component')\n",
        "axes[0].set_ylabel('Second Principal Component')\n",
        "axes[0].set_title('True Labels (Feature Space)')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Class')\n",
        "\n",
        "# Predicted labels\n",
        "scatter2 = axes[1].scatter(X_features_pca[:, 0], X_features_pca[:, 1],\n",
        "                           c=labels_features, cmap='viridis', alpha=0.7, s=50, edgecolors='k')\n",
        "axes[1].set_xlabel('First Principal Component')\n",
        "axes[1].set_ylabel('Second Principal Component')\n",
        "axes[1].set_title('K-Means Clustering (Feature Space)')\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
        "\n",
        "plt.suptitle('PCA Visualization of Extracted Features', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa1_wZKl3LnV"
      },
      "source": [
        "## 5. Real-World Example: Stock Price Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3HXKBpy3LnV"
      },
      "outputs": [],
      "source": [
        "# Download stock data\n",
        "import yfinance as yf\n",
        "\n",
        "# List of stock tickers (diverse sectors)\n",
        "tickers = [\n",
        "    # Technology\n",
        "    'AAPL', 'MSFT', 'GOOGL', 'META', 'NVDA',\n",
        "    # Finance\n",
        "    'JPM', 'BAC', 'GS', 'MS', 'WFC',\n",
        "    # Healthcare\n",
        "    'JNJ', 'PFE', 'UNH', 'MRK', 'ABBV',\n",
        "    # Consumer\n",
        "    'AMZN', 'WMT', 'HD', 'MCD', 'NKE',\n",
        "    # Energy\n",
        "    'XOM', 'CVX', 'COP', 'SLB', 'EOG'\n",
        "]\n",
        "\n",
        "sector_mapping = {\n",
        "    'AAPL': 'Tech', 'MSFT': 'Tech', 'GOOGL': 'Tech', 'META': 'Tech', 'NVDA': 'Tech',\n",
        "    'JPM': 'Finance', 'BAC': 'Finance', 'GS': 'Finance', 'MS': 'Finance', 'WFC': 'Finance',\n",
        "    'JNJ': 'Healthcare', 'PFE': 'Healthcare', 'UNH': 'Healthcare', 'MRK': 'Healthcare', 'ABBV': 'Healthcare',\n",
        "    'AMZN': 'Consumer', 'WMT': 'Consumer', 'HD': 'Consumer', 'MCD': 'Consumer', 'NKE': 'Consumer',\n",
        "    'XOM': 'Energy', 'CVX': 'Energy', 'COP': 'Energy', 'SLB': 'Energy', 'EOG': 'Energy'\n",
        "}\n",
        "\n",
        "print(\"Downloading stock data...\")\n",
        "\n",
        "# Download historical data\n",
        "stock_data = yf.download(tickers, start='2023-01-01', end='2024-01-01', progress=False)\n",
        "\n",
        "# Get adjusted close prices\n",
        "close_prices = stock_data['Adj Close']\n",
        "\n",
        "# Drop any stocks with missing data\n",
        "close_prices = close_prices.dropna(axis=1)\n",
        "\n",
        "print(f\"\\nStock data shape: {close_prices.shape}\")\n",
        "print(f\"Stocks included: {list(close_prices.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYHDBfc13LnV"
      },
      "outputs": [],
      "source": [
        "# Calculate daily returns\n",
        "returns = close_prices.pct_change().dropna()\n",
        "\n",
        "# Normalize prices for comparison (relative to starting price)\n",
        "normalized_prices = close_prices / close_prices.iloc[0] * 100\n",
        "\n",
        "# Visualize stock prices by sector\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "sectors = ['Tech', 'Finance', 'Healthcare', 'Consumer', 'Energy']\n",
        "colors_sector = {'Tech': 'blue', 'Finance': 'green', 'Healthcare': 'red',\n",
        "                 'Consumer': 'orange', 'Energy': 'purple'}\n",
        "\n",
        "for idx, sector in enumerate(sectors):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    sector_stocks = [s for s in close_prices.columns if sector_mapping.get(s) == sector]\n",
        "\n",
        "    for stock in sector_stocks:\n",
        "        ax.plot(normalized_prices[stock], label=stock, alpha=0.7)\n",
        "\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Normalized Price')\n",
        "    ax.set_title(f'{sector} Sector')\n",
        "    ax.legend(loc='upper left', fontsize=8)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "axes[1, 2].axis('off')  # Hide empty subplot\n",
        "\n",
        "plt.suptitle('Normalized Stock Prices by Sector', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMZ3dUL3LnV"
      },
      "outputs": [],
      "source": [
        "# Prepare stock data for clustering\n",
        "X_stocks = normalized_prices.T.values  # Shape: (n_stocks, n_timestamps)\n",
        "stock_names = list(normalized_prices.columns)\n",
        "true_sectors = [sector_mapping.get(s, 'Unknown') for s in stock_names]\n",
        "\n",
        "# Encode sectors for evaluation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_sectors = le.fit_transform(true_sectors)\n",
        "\n",
        "print(f\"Stock time series shape: {X_stocks.shape}\")\n",
        "print(f\"Sectors: {le.classes_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRWplsxN3LnW"
      },
      "outputs": [],
      "source": [
        "# Apply clustering to stocks\n",
        "# Reshape for tslearn\n",
        "X_stocks_reshaped = X_stocks.reshape(X_stocks.shape[0], X_stocks.shape[1], 1)\n",
        "\n",
        "# Normalize\n",
        "scaler_stocks = TimeSeriesScalerMeanVariance()\n",
        "X_stocks_scaled = scaler_stocks.fit_transform(X_stocks_reshaped)\n",
        "\n",
        "# DTW K-Means\n",
        "print(\"Clustering stocks with DTW K-Means...\")\n",
        "dtw_stock = TimeSeriesKMeans(\n",
        "    n_clusters=5,  # 5 sectors\n",
        "    metric='dtw',\n",
        "    max_iter=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "stock_labels_dtw = dtw_stock.fit_predict(X_stocks_scaled)\n",
        "\n",
        "# Euclidean K-Means\n",
        "euclidean_stock = TimeSeriesKMeans(\n",
        "    n_clusters=5,\n",
        "    metric='euclidean',\n",
        "    random_state=42\n",
        ")\n",
        "stock_labels_euclidean = euclidean_stock.fit_predict(X_stocks_scaled)\n",
        "\n",
        "print(\"Clustering complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8oJcR_P3LnW"
      },
      "outputs": [],
      "source": [
        "# Evaluate stock clustering\n",
        "print(\"Stock Clustering Evaluation:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X_stocks_flat = X_stocks_scaled.reshape(X_stocks_scaled.shape[0], -1)\n",
        "\n",
        "for name, labels in [('DTW K-Means', stock_labels_dtw),\n",
        "                     ('Euclidean K-Means', stock_labels_euclidean)]:\n",
        "    ari = adjusted_rand_score(y_sectors, labels)\n",
        "    nmi = normalized_mutual_info_score(y_sectors, labels)\n",
        "    sil = silhouette_score(X_stocks_flat, labels)\n",
        "    print(f\"{name:20s}: ARI={ari:.4f}, NMI={nmi:.4f}, Silhouette={sil:.4f}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUJ2dx563LnW"
      },
      "outputs": [],
      "source": [
        "# Visualize clustering results\n",
        "results_df = pd.DataFrame({\n",
        "    'Stock': stock_names,\n",
        "    'True Sector': true_sectors,\n",
        "    'DTW Cluster': stock_labels_dtw,\n",
        "    'Euclidean Cluster': stock_labels_euclidean\n",
        "})\n",
        "\n",
        "print(\"Stock Clustering Results:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91gSSFnL3LnW"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap showing cluster assignments\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Confusion matrix: True Sector vs DTW Cluster\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm_dtw = pd.crosstab(results_df['True Sector'], results_df['DTW Cluster'])\n",
        "sns.heatmap(cm_dtw, annot=True, cmap='Blues', ax=axes[0], fmt='d')\n",
        "axes[0].set_title('True Sector vs DTW Cluster')\n",
        "axes[0].set_xlabel('DTW Cluster')\n",
        "axes[0].set_ylabel('True Sector')\n",
        "\n",
        "cm_euclidean = pd.crosstab(results_df['True Sector'], results_df['Euclidean Cluster'])\n",
        "sns.heatmap(cm_euclidean, annot=True, cmap='Blues', ax=axes[1], fmt='d')\n",
        "axes[1].set_title('True Sector vs Euclidean Cluster')\n",
        "axes[1].set_xlabel('Euclidean Cluster')\n",
        "axes[1].set_ylabel('True Sector')\n",
        "\n",
        "plt.suptitle('Stock Sector vs Cluster Assignments', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8YYiGtp3LnW"
      },
      "source": [
        "## 6. Hierarchical Clustering with DTW Distance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkjqd8SV3LnW"
      },
      "outputs": [],
      "source": [
        "# Calculate DTW distance matrix for stocks\n",
        "print(\"Calculating DTW distance matrix...\")\n",
        "dtw_distance_matrix = cdist_dtw(X_stocks_scaled)\n",
        "\n",
        "print(f\"Distance matrix shape: {dtw_distance_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_SXH12B3LnW"
      },
      "outputs": [],
      "source": [
        "# Create dendrogram\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "# Convert to condensed distance matrix\n",
        "condensed_dist = squareform(dtw_distance_matrix, checks=False)\n",
        "\n",
        "# Hierarchical clustering\n",
        "Z = linkage(condensed_dist, method='average')\n",
        "\n",
        "# Create dendrogram\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Color by sector\n",
        "sector_colors = {'Tech': 'blue', 'Finance': 'green', 'Healthcare': 'red',\n",
        "                 'Consumer': 'orange', 'Energy': 'purple'}\n",
        "label_colors = [sector_colors.get(sector_mapping.get(s, 'Unknown'), 'black') for s in stock_names]\n",
        "\n",
        "dendrogram(Z, labels=stock_names, leaf_rotation=45, leaf_font_size=10)\n",
        "plt.title('Hierarchical Clustering of Stocks (DTW Distance)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Stock')\n",
        "plt.ylabel('DTW Distance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLegend: Blue=Tech, Green=Finance, Red=Healthcare, Orange=Consumer, Purple=Energy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dG3CNhm3LnW"
      },
      "source": [
        "## 7. Summary and Quality Metrics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXnYAXo83LnW"
      },
      "outputs": [],
      "source": [
        "# Comprehensive comparison of all methods\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE CLUSTERING QUALITY METRICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Synthetic data results\n",
        "print(\"\\n1. SYNTHETIC TIME SERIES DATA:\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "synthetic_results = [\n",
        "    ('DTW K-Means', labels_dtw),\n",
        "    ('Euclidean K-Means', labels_euclidean),\n",
        "    ('Soft-DTW K-Means', labels_soft_dtw),\n",
        "    ('Feature K-Means', labels_features),\n",
        "    ('Feature Hierarchical', labels_hclust)\n",
        "]\n",
        "\n",
        "synthetic_metrics = []\n",
        "for name, labels in synthetic_results:\n",
        "    ari = adjusted_rand_score(y_ts, labels)\n",
        "    nmi = normalized_mutual_info_score(y_ts, labels)\n",
        "    sil = silhouette_score(X_flat, labels)\n",
        "    ch = calinski_harabasz_score(X_flat, labels)\n",
        "    db = davies_bouldin_score(X_flat, labels)\n",
        "\n",
        "    synthetic_metrics.append({\n",
        "        'Method': name,\n",
        "        'ARI': ari,\n",
        "        'NMI': nmi,\n",
        "        'Silhouette': sil,\n",
        "        'CH Index': ch,\n",
        "        'DB Index': db\n",
        "    })\n",
        "\n",
        "syn_df = pd.DataFrame(synthetic_metrics)\n",
        "print(syn_df.to_string(index=False))\n",
        "\n",
        "# Stock data results\n",
        "print(\"\\n2. STOCK TIME SERIES DATA:\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "stock_results = [\n",
        "    ('DTW K-Means', stock_labels_dtw),\n",
        "    ('Euclidean K-Means', stock_labels_euclidean)\n",
        "]\n",
        "\n",
        "stock_metrics = []\n",
        "for name, labels in stock_results:\n",
        "    ari = adjusted_rand_score(y_sectors, labels)\n",
        "    nmi = normalized_mutual_info_score(y_sectors, labels)\n",
        "    sil = silhouette_score(X_stocks_flat, labels)\n",
        "\n",
        "    stock_metrics.append({\n",
        "        'Method': name,\n",
        "        'ARI': ari,\n",
        "        'NMI': nmi,\n",
        "        'Silhouette': sil\n",
        "    })\n",
        "\n",
        "stock_df = pd.DataFrame(stock_metrics)\n",
        "print(stock_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7YkEH7m3LnX"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TIME SERIES CLUSTERING - SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. METHODS USED:\")\n",
        "print(\"   - DTW (Dynamic Time Warping) K-Means\")\n",
        "print(\"   - Euclidean K-Means\")\n",
        "print(\"   - Soft-DTW K-Means\")\n",
        "print(\"   - Feature-based clustering (tsfresh + K-Means)\")\n",
        "print(\"   - Hierarchical clustering with DTW distance\")\n",
        "\n",
        "print(\"\\n2. KEY LIBRARIES:\")\n",
        "print(\"   - tslearn: Time series machine learning\")\n",
        "print(\"   - tsfresh: Automatic feature extraction\")\n",
        "print(\"   - yfinance: Stock data retrieval\")\n",
        "\n",
        "print(\"\\n3. QUALITY METRICS:\")\n",
        "print(\"   - Adjusted Rand Index (ARI): Agreement with ground truth\")\n",
        "print(\"   - Normalized Mutual Information (NMI): Information shared\")\n",
        "print(\"   - Silhouette Score: Cluster cohesion and separation\")\n",
        "print(\"   - Calinski-Harabasz Index: Between/within cluster variance\")\n",
        "print(\"   - Davies-Bouldin Index: Average cluster similarity\")\n",
        "\n",
        "print(\"\\n4. DATASETS:\")\n",
        "print(\"   - Synthetic time series (4 pattern classes)\")\n",
        "print(\"   - Real stock price data (5 sectors)\")\n",
        "\n",
        "print(\"\\n5. KEY FINDINGS:\")\n",
        "best_syn = syn_df.loc[syn_df['ARI'].idxmax()]\n",
        "print(f\"   - Best method for synthetic data: {best_syn['Method']} (ARI: {best_syn['ARI']:.4f})\")\n",
        "print(\"   - DTW captures time warping and phase shifts\")\n",
        "print(\"   - Feature-based methods work well for distinct patterns\")\n",
        "print(\"   - Sector-based stock clustering shows correlation within industries\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j6_q--J3LnX"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Cuturi, M. (2011). Fast global alignment kernels. ICML.\n",
        "2. Sakoe, H., & Chiba, S. (1978). Dynamic programming algorithm optimization for spoken word recognition. IEEE TASSP.\n",
        "3. Christ, M., et al. (2018). Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests (tsfresh). Neurocomputing.\n",
        "4. tslearn documentation: https://tslearn.readthedocs.io/\n",
        "5. tsfresh documentation: https://tsfresh.readthedocs.io/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}