{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85MS-nUx3VG9"
      },
      "source": [
        "# Image Clustering with ImageBind LLM Embeddings\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates image clustering using Meta's ImageBind model, which provides multimodal embeddings. ImageBind can embed images, text, audio, and other modalities into a shared embedding space.\n",
        "\n",
        "## Key Features of ImageBind\n",
        "- **Multimodal Embeddings**: Unified embedding space for multiple modalities\n",
        "- **Zero-shot Transfer**: Works without task-specific training\n",
        "- **State-of-the-art**: Achieves excellent results on image understanding\n",
        "\n",
        "## Alternative: Using CLIP Embeddings\n",
        "We also demonstrate clustering with CLIP, a widely-used vision-language model.\n",
        "\n",
        "## Author\n",
        "Created for Machine Learning Course Assignment\n",
        "\n",
        "## References\n",
        "- [ImageBind Paper](https://arxiv.org/abs/2305.05665)\n",
        "- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n",
        "- [Embedding Clustering Introduction](https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1UaN-X93VG_"
      },
      "source": [
        "## 1. Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-fnfTvK3VG_"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install Pillow --quiet\n",
        "!pip install umap-learn --quiet\n",
        "!pip install hdbscan --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import os\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed and plotting style\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Base libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoc-_bOT3VHA"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch and vision models\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import umap\n",
        "import hdbscan\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVEQo9nW3VHA"
      },
      "source": [
        "## 2. Load CLIP Model (Alternative to ImageBind)\n",
        "\n",
        "Note: ImageBind requires specific setup. We'll use CLIP as it's more readily available and provides similar multimodal embeddings. The concepts and techniques are directly transferable to ImageBind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nglDKYBM3VHA"
      },
      "outputs": [],
      "source": [
        "# Load CLIP model\n",
        "print(\"Loading CLIP model...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = clip_model.to(device)\n",
        "clip_model.eval()\n",
        "print(\"CLIP model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_wp-3BO3VHB"
      },
      "source": [
        "## 3. Download Sample Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lla4FFI3VHB"
      },
      "outputs": [],
      "source": [
        "# Create image URLs for different categories\n",
        "# Using placeholder images that represent different categories\n",
        "\n",
        "# We'll use CIFAR-10 dataset for a reliable image source\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "\n",
        "# Download CIFAR-10\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "cifar_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# CIFAR-10 class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "print(f\"Dataset loaded! Total images: {len(cifar_dataset)}\")\n",
        "print(f\"Classes: {class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afxH7Whl3VHB"
      },
      "outputs": [],
      "source": [
        "# Select a subset of images for clustering (5 classes, 20 images each)\n",
        "selected_classes = [0, 1, 3, 5, 8]  # airplane, automobile, cat, dog, ship\n",
        "selected_class_names = [class_names[i] for i in selected_classes]\n",
        "n_per_class = 20\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "pil_images = []\n",
        "\n",
        "# Get original images without transform for CLIP\n",
        "cifar_raw = datasets.CIFAR10(root='./data', train=False, download=False)\n",
        "\n",
        "# Collect images\n",
        "class_counts = {c: 0 for c in selected_classes}\n",
        "\n",
        "for idx in range(len(cifar_raw)):\n",
        "    img, label = cifar_raw[idx]\n",
        "    if label in selected_classes and class_counts[label] < n_per_class:\n",
        "        pil_images.append(img)\n",
        "        labels.append(selected_classes.index(label))  # Remap to 0-4\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    if all(c >= n_per_class for c in class_counts.values()):\n",
        "        break\n",
        "\n",
        "y_true = np.array(labels)\n",
        "print(f\"Selected {len(pil_images)} images from {len(selected_classes)} classes\")\n",
        "print(f\"Classes: {selected_class_names}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQNLlEXC3VHC"
      },
      "outputs": [],
      "source": [
        "# Visualize sample images from each class\n",
        "fig, axes = plt.subplots(len(selected_classes), 5, figsize=(12, 10))\n",
        "\n",
        "for class_idx, class_name in enumerate(selected_class_names):\n",
        "    class_images = [img for img, lbl in zip(pil_images, labels) if lbl == class_idx][:5]\n",
        "\n",
        "    for img_idx, img in enumerate(class_images):\n",
        "        axes[class_idx, img_idx].imshow(img)\n",
        "        axes[class_idx, img_idx].axis('off')\n",
        "        if img_idx == 0:\n",
        "            axes[class_idx, img_idx].set_ylabel(class_name, fontsize=12)\n",
        "\n",
        "plt.suptitle('Sample Images from Each Category', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ko2ruV3VHC"
      },
      "source": [
        "## 4. Generate Image Embeddings with CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytJhhEmx3VHC"
      },
      "outputs": [],
      "source": [
        "def get_clip_image_embeddings(images, model, processor, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generate CLIP embeddings for a list of PIL images.\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(images), batch_size):\n",
        "        batch_images = images[i:i+batch_size]\n",
        "\n",
        "        # Process images\n",
        "        inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            image_features = model.get_image_features(**inputs)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        all_embeddings.append(image_features.cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"Generating CLIP embeddings...\")\n",
        "image_embeddings = get_clip_image_embeddings(pil_images, clip_model, clip_processor, device)\n",
        "print(f\"Embeddings shape: {image_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wTwvRwQ3VHC"
      },
      "source": [
        "## 5. Visualize Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpogzn7u3VHC"
      },
      "outputs": [],
      "source": [
        "# Reduce dimensions with UMAP for visualization\n",
        "print(\"Reducing dimensions with UMAP...\")\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "embeddings_2d = umap_reducer.fit_transform(image_embeddings)\n",
        "print(\"UMAP reduction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZstJs803VHD"
      },
      "outputs": [],
      "source": [
        "# Visualize embeddings colored by true labels\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "for class_idx, class_name in enumerate(selected_class_names):\n",
        "    mask = y_true == class_idx\n",
        "    plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
        "                c=colors[class_idx], label=class_name,\n",
        "                alpha=0.7, s=80, edgecolors='k')\n",
        "\n",
        "plt.xlabel('UMAP Dimension 1')\n",
        "plt.ylabel('UMAP Dimension 2')\n",
        "plt.title('CLIP Image Embeddings (UMAP Visualization)', fontsize=14, fontweight='bold')\n",
        "plt.legend(title='Category')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxAy6wmA3VHD"
      },
      "source": [
        "## 6. K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5E0GlBx3VHD"
      },
      "outputs": [],
      "source": [
        "# Apply K-Means clustering\n",
        "n_clusters = len(selected_classes)\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "labels_kmeans = kmeans.fit_predict(image_embeddings)\n",
        "\n",
        "# Evaluate\n",
        "ari_kmeans = adjusted_rand_score(y_true, labels_kmeans)\n",
        "nmi_kmeans = normalized_mutual_info_score(y_true, labels_kmeans)\n",
        "sil_kmeans = silhouette_score(image_embeddings, labels_kmeans)\n",
        "\n",
        "print(\"K-Means Clustering Results:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Adjusted Rand Index: {ari_kmeans:.4f}\")\n",
        "print(f\"Normalized Mutual Info: {nmi_kmeans:.4f}\")\n",
        "print(f\"Silhouette Score: {sil_kmeans:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaaK1EYJ3VHD"
      },
      "outputs": [],
      "source": [
        "# Visualize K-Means results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# True labels\n",
        "for class_idx, class_name in enumerate(selected_class_names):\n",
        "    mask = y_true == class_idx\n",
        "    axes[0].scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
        "                    c=colors[class_idx], label=class_name,\n",
        "                    alpha=0.7, s=80, edgecolors='k')\n",
        "axes[0].set_xlabel('UMAP Dimension 1')\n",
        "axes[0].set_ylabel('UMAP Dimension 2')\n",
        "axes[0].set_title('True Labels')\n",
        "axes[0].legend()\n",
        "\n",
        "# K-Means clusters\n",
        "scatter = axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
        "                          c=labels_kmeans, cmap='viridis',\n",
        "                          alpha=0.7, s=80, edgecolors='k')\n",
        "axes[1].set_xlabel('UMAP Dimension 1')\n",
        "axes[1].set_ylabel('UMAP Dimension 2')\n",
        "axes[1].set_title(f'K-Means Clustering\\nARI: {ari_kmeans:.3f}')\n",
        "plt.colorbar(scatter, ax=axes[1], label='Cluster')\n",
        "\n",
        "plt.suptitle('Image Clustering Results', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm53GStF3VHD"
      },
      "source": [
        "## 7. Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP-VowmJ3VHD"
      },
      "outputs": [],
      "source": [
        "# Apply Hierarchical Clustering\n",
        "hclust = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "labels_hclust = hclust.fit_predict(image_embeddings)\n",
        "\n",
        "# Evaluate\n",
        "ari_hclust = adjusted_rand_score(y_true, labels_hclust)\n",
        "nmi_hclust = normalized_mutual_info_score(y_true, labels_hclust)\n",
        "sil_hclust = silhouette_score(image_embeddings, labels_hclust)\n",
        "\n",
        "print(\"Hierarchical Clustering Results:\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Adjusted Rand Index: {ari_hclust:.4f}\")\n",
        "print(f\"Normalized Mutual Info: {nmi_hclust:.4f}\")\n",
        "print(f\"Silhouette Score: {sil_hclust:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aks2k4Xd3VHD"
      },
      "outputs": [],
      "source": [
        "# Create dendrogram\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "Z = linkage(image_embeddings, method='ward')\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Create labels with class names\n",
        "dendrogram_labels = [f\"{selected_class_names[l]}_{i}\" for i, l in enumerate(y_true)]\n",
        "\n",
        "dendrogram(Z, labels=dendrogram_labels, leaf_rotation=90, leaf_font_size=6,\n",
        "           color_threshold=15)\n",
        "plt.title('Hierarchical Clustering Dendrogram (CLIP Embeddings)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Image')\n",
        "plt.ylabel('Distance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ErRObu43VHD"
      },
      "source": [
        "## 8. HDBSCAN Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NEEBQKc3VHD"
      },
      "outputs": [],
      "source": [
        "# Apply HDBSCAN\n",
        "hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\n",
        "labels_hdbscan = hdbscan_clusterer.fit_predict(image_embeddings)\n",
        "\n",
        "n_clusters_found = len(set(labels_hdbscan)) - (1 if -1 in labels_hdbscan else 0)\n",
        "n_noise = (labels_hdbscan == -1).sum()\n",
        "\n",
        "print(f\"HDBSCAN found {n_clusters_found} clusters\")\n",
        "print(f\"Noise points: {n_noise}\")\n",
        "\n",
        "# Evaluate (excluding noise)\n",
        "valid_mask = labels_hdbscan >= 0\n",
        "if valid_mask.sum() > 0 and n_clusters_found > 1:\n",
        "    ari_hdbscan = adjusted_rand_score(y_true[valid_mask], labels_hdbscan[valid_mask])\n",
        "    nmi_hdbscan = normalized_mutual_info_score(y_true[valid_mask], labels_hdbscan[valid_mask])\n",
        "    sil_hdbscan = silhouette_score(image_embeddings[valid_mask], labels_hdbscan[valid_mask])\n",
        "\n",
        "    print(f\"\\nAdjusted Rand Index: {ari_hdbscan:.4f}\")\n",
        "    print(f\"Normalized Mutual Info: {nmi_hdbscan:.4f}\")\n",
        "    print(f\"Silhouette Score: {sil_hdbscan:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBgNx8NX3VHD"
      },
      "source": [
        "## 9. Finding Optimal Number of Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-KbxhRG3VHD"
      },
      "outputs": [],
      "source": [
        "# Elbow method and Silhouette analysis\n",
        "k_range = range(2, 10)\n",
        "inertias = []\n",
        "silhouettes = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels_temp = kmeans_temp.fit_predict(image_embeddings)\n",
        "    inertias.append(kmeans_temp.inertia_)\n",
        "    silhouettes.append(silhouette_score(image_embeddings, labels_temp))\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Elbow curve\n",
        "axes[0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Number of Clusters (K)')\n",
        "axes[0].set_ylabel('Inertia')\n",
        "axes[0].set_title('Elbow Method')\n",
        "axes[0].axvline(x=5, color='red', linestyle='--', label=f'K={n_clusters} (True)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Silhouette curve\n",
        "axes[1].plot(k_range, silhouettes, 'go-', linewidth=2, markersize=8)\n",
        "axes[1].set_xlabel('Number of Clusters (K)')\n",
        "axes[1].set_ylabel('Silhouette Score')\n",
        "axes[1].set_title('Silhouette Analysis')\n",
        "axes[1].axvline(x=5, color='red', linestyle='--', label=f'K={n_clusters} (True)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Optimal Number of Clusters Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "optimal_k = list(k_range)[np.argmax(silhouettes)]\n",
        "print(f\"Optimal K by Silhouette: {optimal_k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIExyjXX3VHD"
      },
      "source": [
        "## 10. Visualize Cluster Contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_NoJNZn3VHD"
      },
      "outputs": [],
      "source": [
        "# Show sample images from each cluster\n",
        "fig, axes = plt.subplots(n_clusters, 5, figsize=(12, 12))\n",
        "\n",
        "for cluster_idx in range(n_clusters):\n",
        "    cluster_mask = labels_kmeans == cluster_idx\n",
        "    cluster_images = [img for img, mask in zip(pil_images, cluster_mask) if mask]\n",
        "    cluster_true_labels = y_true[cluster_mask]\n",
        "\n",
        "    # Get majority class\n",
        "    if len(cluster_true_labels) > 0:\n",
        "        majority_class = selected_class_names[np.bincount(cluster_true_labels).argmax()]\n",
        "    else:\n",
        "        majority_class = \"Empty\"\n",
        "\n",
        "    for img_idx in range(min(5, len(cluster_images))):\n",
        "        axes[cluster_idx, img_idx].imshow(cluster_images[img_idx])\n",
        "        axes[cluster_idx, img_idx].axis('off')\n",
        "\n",
        "    # Fill remaining slots if less than 5 images\n",
        "    for img_idx in range(len(cluster_images), 5):\n",
        "        axes[cluster_idx, img_idx].axis('off')\n",
        "\n",
        "    axes[cluster_idx, 0].set_ylabel(f'Cluster {cluster_idx}\\n({majority_class})',\n",
        "                                     fontsize=10, rotation=0, ha='right', va='center')\n",
        "\n",
        "plt.suptitle('Sample Images from Each Cluster (K-Means)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGUZ0-wK3VHD"
      },
      "source": [
        "## 11. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsNoH6p_3VHE"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_true, labels_kmeans)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(n_clusters), yticklabels=selected_class_names)\n",
        "plt.xlabel('Predicted Cluster')\n",
        "plt.ylabel('True Category')\n",
        "plt.title('Confusion Matrix: Categories vs Clusters', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKwXA5jH3VHE"
      },
      "source": [
        "## 12. Image Similarity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hRfr2co3VHE"
      },
      "outputs": [],
      "source": [
        "# Compute cosine similarity matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(image_embeddings)\n",
        "\n",
        "# Sort by true labels for better visualization\n",
        "sorted_indices = np.argsort(y_true)\n",
        "sorted_similarity = similarity_matrix[sorted_indices][:, sorted_indices]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(sorted_similarity, cmap='RdYlBu_r', vmin=0, vmax=1)\n",
        "\n",
        "# Add category separators\n",
        "n_per = n_per_class\n",
        "for i in range(1, n_clusters):\n",
        "    plt.axhline(y=i*n_per, color='black', linewidth=2)\n",
        "    plt.axvline(x=i*n_per, color='black', linewidth=2)\n",
        "\n",
        "plt.title('Image Similarity Matrix (CLIP Embeddings)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Image (sorted by category)')\n",
        "plt.ylabel('Image (sorted by category)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Diagonal blocks show high similarity within categories.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0cK1KSz3VHE"
      },
      "source": [
        "## 13. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq64lFUj3VHE"
      },
      "outputs": [],
      "source": [
        "# Comprehensive results summary\n",
        "results = pd.DataFrame([\n",
        "    {'Method': 'K-Means', 'ARI': ari_kmeans, 'NMI': nmi_kmeans, 'Silhouette': sil_kmeans},\n",
        "    {'Method': 'Hierarchical', 'ARI': ari_hclust, 'NMI': nmi_hclust, 'Silhouette': sil_hclust},\n",
        "])\n",
        "\n",
        "if valid_mask.sum() > 0 and n_clusters_found > 1:\n",
        "    results = pd.concat([results, pd.DataFrame([{\n",
        "        'Method': 'HDBSCAN', 'ARI': ari_hdbscan, 'NMI': nmi_hdbscan, 'Silhouette': sil_hdbscan\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"IMAGE CLUSTERING RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAll Methods Comparison:\")\n",
        "print(results.to_string(index=False))\n",
        "\n",
        "# Best method\n",
        "best_method = results.loc[results['ARI'].idxmax()]\n",
        "print(f\"\\nBest Method: {best_method['Method']}\")\n",
        "print(f\"ARI: {best_method['ARI']:.4f}, NMI: {best_method['NMI']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHqNFBgL3VHE"
      },
      "outputs": [],
      "source": [
        "# Visualize results comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(results))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, results['ARI'], width, label='ARI', color='steelblue')\n",
        "bars2 = ax.bar(x, results['NMI'], width, label='NMI', color='coral')\n",
        "bars3 = ax.bar(x + width, results['Silhouette'], width, label='Silhouette', color='green')\n",
        "\n",
        "ax.set_xlabel('Clustering Method')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Image Clustering Methods Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(results['Method'])\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX1MuWXO3VHE"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"IMAGE CLUSTERING WITH LLM EMBEDDINGS - SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. EMBEDDING MODEL:\")\n",
        "print(\"   - CLIP (clip-vit-base-patch32): Vision-language model\")\n",
        "print(\"   - Produces 512-dimensional embeddings\")\n",
        "print(\"   - Note: ImageBind follows similar principles for multimodal embeddings\")\n",
        "\n",
        "print(\"\\n2. CLUSTERING ALGORITHMS:\")\n",
        "print(\"   - K-Means: Partition-based clustering\")\n",
        "print(\"   - Hierarchical: Agglomerative with Ward linkage\")\n",
        "print(\"   - HDBSCAN: Density-based clustering\")\n",
        "\n",
        "print(\"\\n3. QUALITY METRICS:\")\n",
        "print(\"   - Adjusted Rand Index (ARI): Agreement with ground truth\")\n",
        "print(\"   - Normalized Mutual Information (NMI): Information overlap\")\n",
        "print(\"   - Silhouette Score: Cluster cohesion and separation\")\n",
        "\n",
        "print(\"\\n4. DATASET:\")\n",
        "print(f\"   - CIFAR-10 subset: {len(pil_images)} images\")\n",
        "print(f\"   - {len(selected_classes)} classes: {', '.join(selected_class_names)}\")\n",
        "\n",
        "print(\"\\n5. KEY FINDINGS:\")\n",
        "print(f\"   - Best method: {best_method['Method']} (ARI: {best_method['ARI']:.4f})\")\n",
        "print(\"   - CLIP embeddings effectively capture visual semantics\")\n",
        "print(\"   - Similar images cluster together in embedding space\")\n",
        "print(\"   - Hierarchical clustering provides interpretable structure\")\n",
        "\n",
        "print(\"\\n6. APPLICATIONS:\")\n",
        "print(\"   - Image organization and retrieval\")\n",
        "print(\"   - Visual content recommendation\")\n",
        "print(\"   - Duplicate detection\")\n",
        "print(\"   - Dataset curation and cleaning\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJnSjkSk3VHE"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Girdhar, R., et al. (2023). ImageBind: One Embedding Space To Bind Them All. CVPR.\n",
        "2. Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. ICML.\n",
        "3. McInnes, L., & Healy, J. (2017). HDBSCAN: Hierarchical density based clustering.\n",
        "4. CLIP Documentation: https://github.com/openai/CLIP\n",
        "5. ImageBind: https://github.com/facebookresearch/ImageBind"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}