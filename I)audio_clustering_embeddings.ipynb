{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdD5OmbL3Vp4"
      },
      "source": [
        "# Audio Clustering with Deep Learning Embeddings\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates audio clustering using deep learning embeddings. We'll explore how modern audio models can capture acoustic features and enable effective audio clustering.\n",
        "\n",
        "## Approaches Covered\n",
        "1. **Traditional Audio Features**: MFCCs, spectral features\n",
        "2. **Pretrained Audio Embeddings**: Using models like Wav2Vec2, HuBERT\n",
        "3. **CLAP Embeddings**: Audio-language model similar to CLIP\n",
        "\n",
        "## Key Concepts\n",
        "- **MFCC**: Mel-frequency cepstral coefficients\n",
        "- **Spectrograms**: Time-frequency representations\n",
        "- **Audio Embeddings**: Learned representations from neural networks\n",
        "\n",
        "## Author\n",
        "Created for Machine Learning Course Assignment\n",
        "\n",
        "## References\n",
        "- [Audio Clustering with Deep Learning](https://ridakhan5.medium.com/audio-clustering-with-deep-learning-a7991d605fa5)\n",
        "- [K-Means Clustering for Music](https://towardsdatascience.com/k-means-clustering-and-pca-to-categorize-music-by-similar-audio-features-df09c93e8b64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5IeUubW3Vp6"
      },
      "source": [
        "## 1. Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85o_fq803Vp6"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install librosa --quiet\n",
        "!pip install soundfile --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install torch torchaudio --quiet\n",
        "!pip install umap-learn --quiet\n",
        "!pip install hdbscan --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",
        "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed and plotting style\n",
        "np.random.seed(42)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "print(\"Base libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLcwaBff3Vp6"
      },
      "outputs": [],
      "source": [
        "# Import audio processing libraries\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "import umap\n",
        "import hdbscan\n",
        "\n",
        "# Check device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Librosa version: {librosa.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6imCHlF3Vp6"
      },
      "source": [
        "## 2. Generate Synthetic Audio Data\n",
        "\n",
        "Since we need audio files for clustering, we'll generate synthetic audio samples representing different categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Web867223Vp7"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_audio(category, duration=2.0, sr=16000):\n",
        "    \"\"\"\n",
        "    Generate synthetic audio samples for different categories.\n",
        "\n",
        "    Categories:\n",
        "    - 'tone': Pure sine wave tones\n",
        "    - 'noise': White/pink noise\n",
        "    - 'chirp': Frequency sweeps\n",
        "    - 'harmonics': Multi-harmonic sounds\n",
        "    - 'pulse': Rhythmic pulses\n",
        "    \"\"\"\n",
        "    t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
        "\n",
        "    if category == 'tone':\n",
        "        # Pure sine wave with slight variations\n",
        "        freq = np.random.uniform(200, 800)\n",
        "        audio = np.sin(2 * np.pi * freq * t)\n",
        "        audio *= 0.5  # Normalize\n",
        "\n",
        "    elif category == 'noise':\n",
        "        # Filtered noise\n",
        "        audio = np.random.randn(len(t))\n",
        "        # Simple low-pass filter effect\n",
        "        filter_strength = np.random.uniform(0.8, 0.95)\n",
        "        for i in range(1, len(audio)):\n",
        "            audio[i] = filter_strength * audio[i-1] + (1-filter_strength) * audio[i]\n",
        "        audio = audio / np.max(np.abs(audio)) * 0.5\n",
        "\n",
        "    elif category == 'chirp':\n",
        "        # Frequency sweep\n",
        "        f0 = np.random.uniform(100, 300)\n",
        "        f1 = np.random.uniform(800, 1500)\n",
        "        phase = 2 * np.pi * (f0 * t + (f1 - f0) * t**2 / (2 * duration))\n",
        "        audio = np.sin(phase)\n",
        "        audio *= 0.5\n",
        "\n",
        "    elif category == 'harmonics':\n",
        "        # Multi-harmonic sound\n",
        "        fundamental = np.random.uniform(150, 300)\n",
        "        audio = np.zeros(len(t))\n",
        "        for h in range(1, 6):\n",
        "            amplitude = 1.0 / h\n",
        "            audio += amplitude * np.sin(2 * np.pi * fundamental * h * t)\n",
        "        audio = audio / np.max(np.abs(audio)) * 0.5\n",
        "\n",
        "    elif category == 'pulse':\n",
        "        # Rhythmic pulses\n",
        "        rate = np.random.uniform(4, 10)  # pulses per second\n",
        "        audio = np.zeros(len(t))\n",
        "        pulse_freq = np.random.uniform(300, 600)\n",
        "        for pulse_time in np.arange(0, duration, 1/rate):\n",
        "            idx = int(pulse_time * sr)\n",
        "            pulse_duration = int(0.05 * sr)  # 50ms pulse\n",
        "            if idx + pulse_duration < len(audio):\n",
        "                envelope = np.exp(-np.linspace(0, 5, pulse_duration))\n",
        "                pulse_t = np.linspace(0, 0.05, pulse_duration)\n",
        "                audio[idx:idx+pulse_duration] += envelope * np.sin(2 * np.pi * pulse_freq * pulse_t)\n",
        "        audio = audio / np.max(np.abs(audio) + 1e-8) * 0.5\n",
        "\n",
        "    # Add small amount of noise for realism\n",
        "    audio += np.random.randn(len(audio)) * 0.01\n",
        "\n",
        "    return audio.astype(np.float32)\n",
        "\n",
        "# Generate samples\n",
        "categories = ['tone', 'noise', 'chirp', 'harmonics', 'pulse']\n",
        "n_samples_per_category = 20\n",
        "sr = 16000\n",
        "duration = 2.0\n",
        "\n",
        "audio_samples = []\n",
        "labels = []\n",
        "\n",
        "for cat_idx, category in enumerate(categories):\n",
        "    for _ in range(n_samples_per_category):\n",
        "        audio = generate_synthetic_audio(category, duration, sr)\n",
        "        audio_samples.append(audio)\n",
        "        labels.append(cat_idx)\n",
        "\n",
        "audio_samples = np.array(audio_samples)\n",
        "y_true = np.array(labels)\n",
        "\n",
        "print(f\"Generated {len(audio_samples)} audio samples\")\n",
        "print(f\"Categories: {categories}\")\n",
        "print(f\"Samples per category: {n_samples_per_category}\")\n",
        "print(f\"Sample rate: {sr} Hz\")\n",
        "print(f\"Duration: {duration} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvNZCbqJ3Vp7"
      },
      "outputs": [],
      "source": [
        "# Visualize sample waveforms from each category\n",
        "fig, axes = plt.subplots(len(categories), 2, figsize=(14, 12))\n",
        "\n",
        "for cat_idx, category in enumerate(categories):\n",
        "    # Get first sample from this category\n",
        "    sample_idx = cat_idx * n_samples_per_category\n",
        "    audio = audio_samples[sample_idx]\n",
        "\n",
        "    # Waveform\n",
        "    t = np.linspace(0, duration, len(audio))\n",
        "    axes[cat_idx, 0].plot(t[:1000], audio[:1000], 'b-', linewidth=0.5)\n",
        "    axes[cat_idx, 0].set_ylabel(category.capitalize())\n",
        "    axes[cat_idx, 0].set_xlim([0, t[1000]])\n",
        "    if cat_idx == 0:\n",
        "        axes[cat_idx, 0].set_title('Waveform')\n",
        "    if cat_idx == len(categories) - 1:\n",
        "        axes[cat_idx, 0].set_xlabel('Time (s)')\n",
        "\n",
        "    # Spectrogram\n",
        "    D = librosa.stft(audio)\n",
        "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz', ax=axes[cat_idx, 1])\n",
        "    if cat_idx == 0:\n",
        "        axes[cat_idx, 1].set_title('Spectrogram')\n",
        "    if cat_idx < len(categories) - 1:\n",
        "        axes[cat_idx, 1].set_xlabel('')\n",
        "\n",
        "plt.suptitle('Sample Audio Visualizations by Category', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niIl-DEV3Vp8"
      },
      "source": [
        "## 3. Extract Traditional Audio Features (MFCCs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia52Cs2B3Vp8"
      },
      "outputs": [],
      "source": [
        "def extract_mfcc_features(audio, sr, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from audio.\n",
        "    Returns mean and std of each MFCC coefficient.\n",
        "    \"\"\"\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
        "    mfcc_mean = np.mean(mfccs, axis=1)\n",
        "    mfcc_std = np.std(mfccs, axis=1)\n",
        "    return np.concatenate([mfcc_mean, mfcc_std])\n",
        "\n",
        "def extract_spectral_features(audio, sr):\n",
        "    \"\"\"\n",
        "    Extract spectral features from audio.\n",
        "    \"\"\"\n",
        "    # Spectral centroid\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
        "\n",
        "    # Spectral bandwidth\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
        "\n",
        "    # Spectral rolloff\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
        "\n",
        "    # Zero crossing rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(audio)\n",
        "\n",
        "    # RMS energy\n",
        "    rms = librosa.feature.rms(y=audio)\n",
        "\n",
        "    features = np.array([\n",
        "        np.mean(spectral_centroid), np.std(spectral_centroid),\n",
        "        np.mean(spectral_bandwidth), np.std(spectral_bandwidth),\n",
        "        np.mean(spectral_rolloff), np.std(spectral_rolloff),\n",
        "        np.mean(zcr), np.std(zcr),\n",
        "        np.mean(rms), np.std(rms)\n",
        "    ])\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for all samples\n",
        "print(\"Extracting MFCC features...\")\n",
        "mfcc_features = np.array([extract_mfcc_features(audio, sr) for audio in audio_samples])\n",
        "print(f\"MFCC features shape: {mfcc_features.shape}\")\n",
        "\n",
        "print(\"\\nExtracting spectral features...\")\n",
        "spectral_features = np.array([extract_spectral_features(audio, sr) for audio in audio_samples])\n",
        "print(f\"Spectral features shape: {spectral_features.shape}\")\n",
        "\n",
        "# Combine all traditional features\n",
        "traditional_features = np.hstack([mfcc_features, spectral_features])\n",
        "print(f\"\\nCombined traditional features shape: {traditional_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgMZrcCH3Vp8"
      },
      "source": [
        "## 4. Extract Deep Learning Embeddings (Wav2Vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRama6RA3Vp8"
      },
      "outputs": [],
      "source": [
        "# Load Wav2Vec2 model\n",
        "print(\"Loading Wav2Vec2 model...\")\n",
        "wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "wav2vec_model = wav2vec_model.to(device)\n",
        "wav2vec_model.eval()\n",
        "print(\"Wav2Vec2 model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvc_jU6g3Vp8"
      },
      "outputs": [],
      "source": [
        "def get_wav2vec_embeddings(audio_samples, processor, model, device, sr=16000):\n",
        "    \"\"\"\n",
        "    Extract Wav2Vec2 embeddings from audio samples.\n",
        "    Returns mean-pooled embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    for audio in audio_samples:\n",
        "        # Process audio\n",
        "        inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Mean pooling\n",
        "        embedding = hidden_states.mean(dim=1).squeeze().cpu().numpy()\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Extract embeddings\n",
        "print(\"Extracting Wav2Vec2 embeddings (this may take a moment)...\")\n",
        "wav2vec_embeddings = get_wav2vec_embeddings(audio_samples, wav2vec_processor, wav2vec_model, device, sr)\n",
        "print(f\"Wav2Vec2 embeddings shape: {wav2vec_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oORgnhh33Vp8"
      },
      "source": [
        "## 5. Visualize Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KitskapV3Vp8"
      },
      "outputs": [],
      "source": [
        "# Normalize features\n",
        "scaler_trad = StandardScaler()\n",
        "traditional_features_scaled = scaler_trad.fit_transform(traditional_features)\n",
        "\n",
        "scaler_wav2vec = StandardScaler()\n",
        "wav2vec_embeddings_scaled = scaler_wav2vec.fit_transform(wav2vec_embeddings)\n",
        "\n",
        "# UMAP reduction\n",
        "print(\"Reducing dimensions with UMAP...\")\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "\n",
        "traditional_2d = umap_reducer.fit_transform(traditional_features_scaled)\n",
        "wav2vec_2d = umap_reducer.fit_transform(wav2vec_embeddings_scaled)\n",
        "\n",
        "print(\"UMAP reduction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhuwkIqy3Vp9"
      },
      "outputs": [],
      "source": [
        "# Visualize embeddings\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "# Traditional features\n",
        "for cat_idx, category in enumerate(categories):\n",
        "    mask = y_true == cat_idx\n",
        "    axes[0].scatter(traditional_2d[mask, 0], traditional_2d[mask, 1],\n",
        "                    c=colors[cat_idx], label=category.capitalize(),\n",
        "                    alpha=0.7, s=80, edgecolors='k')\n",
        "axes[0].set_xlabel('UMAP Dimension 1')\n",
        "axes[0].set_ylabel('UMAP Dimension 2')\n",
        "axes[0].set_title('Traditional Features (MFCC + Spectral)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Wav2Vec2 embeddings\n",
        "for cat_idx, category in enumerate(categories):\n",
        "    mask = y_true == cat_idx\n",
        "    axes[1].scatter(wav2vec_2d[mask, 0], wav2vec_2d[mask, 1],\n",
        "                    c=colors[cat_idx], label=category.capitalize(),\n",
        "                    alpha=0.7, s=80, edgecolors='k')\n",
        "axes[1].set_xlabel('UMAP Dimension 1')\n",
        "axes[1].set_ylabel('UMAP Dimension 2')\n",
        "axes[1].set_title('Wav2Vec2 Embeddings')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.suptitle('Audio Embeddings Visualization (UMAP)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okTp0Oxv3Vp9"
      },
      "source": [
        "## 6. K-Means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjhtIgwc3Vp9"
      },
      "outputs": [],
      "source": [
        "# Apply K-Means to both feature sets\n",
        "n_clusters = len(categories)\n",
        "\n",
        "# Traditional features\n",
        "kmeans_trad = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "labels_trad = kmeans_trad.fit_predict(traditional_features_scaled)\n",
        "\n",
        "# Wav2Vec2 embeddings\n",
        "kmeans_wav2vec = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "labels_wav2vec = kmeans_wav2vec.fit_predict(wav2vec_embeddings_scaled)\n",
        "\n",
        "# Evaluate\n",
        "def evaluate_clustering(features, labels_pred, y_true, method_name):\n",
        "    ari = adjusted_rand_score(y_true, labels_pred)\n",
        "    nmi = normalized_mutual_info_score(y_true, labels_pred)\n",
        "    sil = silhouette_score(features, labels_pred)\n",
        "    ch = calinski_harabasz_score(features, labels_pred)\n",
        "    db = davies_bouldin_score(features, labels_pred)\n",
        "\n",
        "    return {\n",
        "        'Method': method_name,\n",
        "        'ARI': ari,\n",
        "        'NMI': nmi,\n",
        "        'Silhouette': sil,\n",
        "        'CH Index': ch,\n",
        "        'DB Index': db\n",
        "    }\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_clustering(traditional_features_scaled, labels_trad, y_true, 'Traditional + K-Means'))\n",
        "results.append(evaluate_clustering(wav2vec_embeddings_scaled, labels_wav2vec, y_true, 'Wav2Vec2 + K-Means'))\n",
        "\n",
        "print(\"K-Means Clustering Results:\")\n",
        "print(\"=\"*80)\n",
        "print(pd.DataFrame(results).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPH6Tl7z3Vp9"
      },
      "outputs": [],
      "source": [
        "# Visualize clustering results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Traditional + K-Means\n",
        "scatter1 = axes[0].scatter(traditional_2d[:, 0], traditional_2d[:, 1],\n",
        "                           c=labels_trad, cmap='viridis',\n",
        "                           alpha=0.7, s=80, edgecolors='k')\n",
        "axes[0].set_xlabel('UMAP Dimension 1')\n",
        "axes[0].set_ylabel('UMAP Dimension 2')\n",
        "axes[0].set_title(f'Traditional + K-Means\\nARI: {results[0][\"ARI\"]:.3f}')\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
        "\n",
        "# Wav2Vec2 + K-Means\n",
        "scatter2 = axes[1].scatter(wav2vec_2d[:, 0], wav2vec_2d[:, 1],\n",
        "                           c=labels_wav2vec, cmap='viridis',\n",
        "                           alpha=0.7, s=80, edgecolors='k')\n",
        "axes[1].set_xlabel('UMAP Dimension 1')\n",
        "axes[1].set_ylabel('UMAP Dimension 2')\n",
        "axes[1].set_title(f'Wav2Vec2 + K-Means\\nARI: {results[1][\"ARI\"]:.3f}')\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
        "\n",
        "plt.suptitle('Audio Clustering Results', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX7U30OU3Vp9"
      },
      "source": [
        "## 7. Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuamoIhR3Vp9"
      },
      "outputs": [],
      "source": [
        "# Apply Hierarchical Clustering\n",
        "hclust_trad = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "labels_hclust_trad = hclust_trad.fit_predict(traditional_features_scaled)\n",
        "\n",
        "hclust_wav2vec = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "labels_hclust_wav2vec = hclust_wav2vec.fit_predict(wav2vec_embeddings_scaled)\n",
        "\n",
        "# Evaluate\n",
        "results.append(evaluate_clustering(traditional_features_scaled, labels_hclust_trad, y_true, 'Traditional + Hierarchical'))\n",
        "results.append(evaluate_clustering(wav2vec_embeddings_scaled, labels_hclust_wav2vec, y_true, 'Wav2Vec2 + Hierarchical'))\n",
        "\n",
        "print(\"Hierarchical Clustering Results:\")\n",
        "print(\"=\"*80)\n",
        "print(pd.DataFrame(results[2:]).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKwG3MRv3Vp9"
      },
      "outputs": [],
      "source": [
        "# Create dendrogram for Wav2Vec2 embeddings\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "Z = linkage(wav2vec_embeddings_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Create labels\n",
        "dendrogram_labels = [f\"{categories[l]}_{i%n_samples_per_category}\" for i, l in enumerate(y_true)]\n",
        "\n",
        "dendrogram(Z, labels=dendrogram_labels, leaf_rotation=90, leaf_font_size=6,\n",
        "           color_threshold=50)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Wav2Vec2 Embeddings)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Audio Sample')\n",
        "plt.ylabel('Distance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO1YVTMI3Vp9"
      },
      "source": [
        "## 8. HDBSCAN Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN6RTdQl3Vp9"
      },
      "outputs": [],
      "source": [
        "# Apply HDBSCAN\n",
        "hdbscan_trad = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\n",
        "labels_hdbscan_trad = hdbscan_trad.fit_predict(traditional_features_scaled)\n",
        "\n",
        "hdbscan_wav2vec = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)\n",
        "labels_hdbscan_wav2vec = hdbscan_wav2vec.fit_predict(wav2vec_embeddings_scaled)\n",
        "\n",
        "print(\"HDBSCAN Results:\")\n",
        "print(f\"Traditional - Clusters: {len(set(labels_hdbscan_trad)) - (1 if -1 in labels_hdbscan_trad else 0)}, Noise: {(labels_hdbscan_trad == -1).sum()}\")\n",
        "print(f\"Wav2Vec2 - Clusters: {len(set(labels_hdbscan_wav2vec)) - (1 if -1 in labels_hdbscan_wav2vec else 0)}, Noise: {(labels_hdbscan_wav2vec == -1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GENImZFK3Vp9"
      },
      "source": [
        "## 9. Finding Optimal Number of Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiXFmP8x3Vp9"
      },
      "outputs": [],
      "source": [
        "# Elbow method and Silhouette analysis\n",
        "k_range = range(2, 10)\n",
        "inertias_trad = []\n",
        "inertias_wav2vec = []\n",
        "silhouettes_trad = []\n",
        "silhouettes_wav2vec = []\n",
        "\n",
        "for k in k_range:\n",
        "    # Traditional\n",
        "    km_trad = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = km_trad.fit_predict(traditional_features_scaled)\n",
        "    inertias_trad.append(km_trad.inertia_)\n",
        "    silhouettes_trad.append(silhouette_score(traditional_features_scaled, labels))\n",
        "\n",
        "    # Wav2Vec2\n",
        "    km_wav = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = km_wav.fit_predict(wav2vec_embeddings_scaled)\n",
        "    inertias_wav2vec.append(km_wav.inertia_)\n",
        "    silhouettes_wav2vec.append(silhouette_score(wav2vec_embeddings_scaled, labels))\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Elbow - Traditional\n",
        "axes[0, 0].plot(k_range, inertias_trad, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0, 0].axvline(x=5, color='red', linestyle='--', label='K=5 (True)')\n",
        "axes[0, 0].set_xlabel('Number of Clusters (K)')\n",
        "axes[0, 0].set_ylabel('Inertia')\n",
        "axes[0, 0].set_title('Elbow Method - Traditional Features')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Elbow - Wav2Vec2\n",
        "axes[0, 1].plot(k_range, inertias_wav2vec, 'go-', linewidth=2, markersize=8)\n",
        "axes[0, 1].axvline(x=5, color='red', linestyle='--', label='K=5 (True)')\n",
        "axes[0, 1].set_xlabel('Number of Clusters (K)')\n",
        "axes[0, 1].set_ylabel('Inertia')\n",
        "axes[0, 1].set_title('Elbow Method - Wav2Vec2 Embeddings')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Silhouette - Traditional\n",
        "axes[1, 0].plot(k_range, silhouettes_trad, 'bo-', linewidth=2, markersize=8)\n",
        "axes[1, 0].axvline(x=5, color='red', linestyle='--', label='K=5 (True)')\n",
        "axes[1, 0].set_xlabel('Number of Clusters (K)')\n",
        "axes[1, 0].set_ylabel('Silhouette Score')\n",
        "axes[1, 0].set_title('Silhouette Analysis - Traditional Features')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Silhouette - Wav2Vec2\n",
        "axes[1, 1].plot(k_range, silhouettes_wav2vec, 'go-', linewidth=2, markersize=8)\n",
        "axes[1, 1].axvline(x=5, color='red', linestyle='--', label='K=5 (True)')\n",
        "axes[1, 1].set_xlabel('Number of Clusters (K)')\n",
        "axes[1, 1].set_ylabel('Silhouette Score')\n",
        "axes[1, 1].set_title('Silhouette Analysis - Wav2Vec2 Embeddings')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Optimal Number of Clusters Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Optimal K (Traditional) by Silhouette: {list(k_range)[np.argmax(silhouettes_trad)]}\")\n",
        "print(f\"Optimal K (Wav2Vec2) by Silhouette: {list(k_range)[np.argmax(silhouettes_wav2vec)]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ3zXw3U3Vp9"
      },
      "source": [
        "## 10. Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzEqrF7D3Vp9"
      },
      "outputs": [],
      "source": [
        "# Create confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Traditional\n",
        "cm_trad = confusion_matrix(y_true, labels_trad)\n",
        "sns.heatmap(cm_trad, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=range(n_clusters), yticklabels=[c.capitalize() for c in categories])\n",
        "axes[0].set_xlabel('Predicted Cluster')\n",
        "axes[0].set_ylabel('True Category')\n",
        "axes[0].set_title('Traditional Features + K-Means')\n",
        "\n",
        "# Wav2Vec2\n",
        "cm_wav = confusion_matrix(y_true, labels_wav2vec)\n",
        "sns.heatmap(cm_wav, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=range(n_clusters), yticklabels=[c.capitalize() for c in categories])\n",
        "axes[1].set_xlabel('Predicted Cluster')\n",
        "axes[1].set_ylabel('True Category')\n",
        "axes[1].set_title('Wav2Vec2 + K-Means')\n",
        "\n",
        "plt.suptitle('Confusion Matrices: Audio Categories vs Clusters', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUcoZfx3Vp9"
      },
      "source": [
        "## 11. Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nESO4x853Vp9"
      },
      "outputs": [],
      "source": [
        "# Comprehensive results\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AUDIO CLUSTERING RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAll Methods Comparison:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Best method\n",
        "best_method = results_df.loc[results_df['ARI'].idxmax()]\n",
        "print(f\"\\nBest Method: {best_method['Method']}\")\n",
        "print(f\"ARI: {best_method['ARI']:.4f}, NMI: {best_method['NMI']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnRMIgP33Vp9"
      },
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, results_df['ARI'], width, label='ARI', color='steelblue')\n",
        "bars2 = ax.bar(x, results_df['NMI'], width, label='NMI', color='coral')\n",
        "bars3 = ax.bar(x + width, results_df['Silhouette'], width, label='Silhouette', color='green')\n",
        "\n",
        "ax.set_xlabel('Method')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Audio Clustering Methods Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNC-hBuR3Vp9"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"AUDIO CLUSTERING WITH DEEP LEARNING EMBEDDINGS - SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. FEATURE EXTRACTION METHODS:\")\n",
        "print(\"   - Traditional: MFCCs (26 features) + Spectral features (10 features)\")\n",
        "print(\"   - Deep Learning: Wav2Vec2 embeddings (768 dimensions)\")\n",
        "\n",
        "print(\"\\n2. CLUSTERING ALGORITHMS:\")\n",
        "print(\"   - K-Means: Partition-based clustering\")\n",
        "print(\"   - Hierarchical: Agglomerative with Ward linkage\")\n",
        "print(\"   - HDBSCAN: Density-based clustering\")\n",
        "\n",
        "print(\"\\n3. QUALITY METRICS:\")\n",
        "print(\"   - Adjusted Rand Index (ARI): Agreement with ground truth\")\n",
        "print(\"   - Normalized Mutual Information (NMI): Information overlap\")\n",
        "print(\"   - Silhouette Score: Cluster cohesion and separation\")\n",
        "print(\"   - Calinski-Harabasz Index: Variance ratio criterion\")\n",
        "print(\"   - Davies-Bouldin Index: Cluster similarity measure\")\n",
        "\n",
        "print(\"\\n4. DATASET:\")\n",
        "print(f\"   - {len(audio_samples)} synthetic audio samples\")\n",
        "print(f\"   - {len(categories)} categories: {', '.join([c.capitalize() for c in categories])}\")\n",
        "print(f\"   - Sample rate: {sr} Hz, Duration: {duration} seconds\")\n",
        "\n",
        "print(\"\\n5. KEY FINDINGS:\")\n",
        "print(f\"   - Best method: {best_method['Method']}\")\n",
        "print(\"   - Deep learning embeddings capture complex acoustic patterns\")\n",
        "print(\"   - Traditional features work well for distinct audio types\")\n",
        "print(\"   - Wav2Vec2 provides rich semantic representations\")\n",
        "\n",
        "print(\"\\n6. APPLICATIONS:\")\n",
        "print(\"   - Music genre classification\")\n",
        "print(\"   - Speaker diarization\")\n",
        "print(\"   - Sound event detection\")\n",
        "print(\"   - Audio content organization\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_eO3ZbG3Vp-"
      },
      "source": [
        "## References\n",
        "\n",
        "1. Baevski, A., et al. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. NeurIPS.\n",
        "2. Davis, S., & Mermelstein, P. (1980). Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE TASSP.\n",
        "3. McFee, B., et al. (2015). librosa: Audio and Music Signal Analysis in Python. SciPy.\n",
        "4. Wu, Y., et al. (2023). Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation (CLAP).\n",
        "5. Librosa Documentation: https://librosa.org/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}